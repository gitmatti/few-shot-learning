{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from few_shot_learning.datasets import FashionProductImages, FashionProductImagesSmall   \n",
    "from few_shot_learning.utils_evaluation import evaluate_few_shot\n",
    "from config import DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to integrate as much as possible with the existing code from the [github repository](https://github.com/oscarknagg/few-shot). To this end, I adapted mainly the dataset classes `few_shot_learning.datasets.FashionProductImages` and `few_shot_learning.datasets.FashionProductImagesSmall` to expose a `pandas.DataFrame` with attributes `'class_id'` and `'id'` to allow integration with `few_shot.core.NShotTaskSampler` from https://github.com/oscarknagg/few-shot/blob/master/few_shot/core.py. In keeping with the repository's naming conventions, it chose to call the **meta-training set** set the **background set** and the **meta-test set** the **evaluation set**.\n",
    "\n",
    "Further changes to the existing codebase appear in `few_shot_learning.train_few_shot` and are marked as such. They are mainly small bugfixes and concern a different monitoring strategy during training (monitoring of validation accuracy instead of test accuracy).\n",
    "\n",
    "The following code snippets illustrate the functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use split='all' to ignore the traing/test split from the transfer learning experiments\n",
    "# use classes='background' to use the background/meta-training classes\n",
    "background = FashionProductImagesSmall(DATA_PATH, split='all', classes='background')\n",
    "# use classes='background' to use the evaluation/meta-testing classes\n",
    "evaluation = FashionProductImagesSmall(DATA_PATH, split='all', classes='evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "      <th>productDisplayName2</th>\n",
       "      <th>filename</th>\n",
       "      <th>my_id</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59263.jpg</td>\n",
       "      <td>59263</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53759.jpg</td>\n",
       "      <td>53759</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Inkfruit Mens Chain Reaction T-shirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1855.jpg</td>\n",
       "      <td>1855</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Black</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Skagen Men Black Watch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30039.jpg</td>\n",
       "      <td>30039</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Belts</td>\n",
       "      <td>Belts</td>\n",
       "      <td>Black</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Fossil Women Black Huarache Weave Belt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48123.jpg</td>\n",
       "      <td>48123</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44439</td>\n",
       "      <td>25039</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Peach</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Tantra Women Printed Peach T-shirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12544.jpg</td>\n",
       "      <td>12544</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44440</td>\n",
       "      <td>25040</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Sepia Women Blue Printed Top</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42234.jpg</td>\n",
       "      <td>42234</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44442</td>\n",
       "      <td>25041</td>\n",
       "      <td>Men</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Red</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Lotto Men's Soccer Track Flip Flop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6461.jpg</td>\n",
       "      <td>6461</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44443</td>\n",
       "      <td>25042</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Graphic Stellar Blue Tshirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18842.jpg</td>\n",
       "      <td>18842</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44445</td>\n",
       "      <td>25043</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Pink</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Fossil Women Pink Dial Chronograph Watch ES3050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51623.jpg</td>\n",
       "      <td>51623</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25044 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender masterCategory subCategory articleType baseColour  season  \\\n",
       "2          0  Women    Accessories     Watches     Watches     Silver  Winter   \n",
       "4          1    Men        Apparel     Topwear     Tshirts       Grey  Summer   \n",
       "5          2    Men        Apparel     Topwear     Tshirts       Grey  Summer   \n",
       "9          3    Men    Accessories     Watches     Watches      Black  Winter   \n",
       "11         4  Women    Accessories       Belts       Belts      Black  Summer   \n",
       "...      ...    ...            ...         ...         ...        ...     ...   \n",
       "44439  25039  Women        Apparel     Topwear     Tshirts      Peach    Fall   \n",
       "44440  25040  Women        Apparel     Topwear        Tops       Blue  Summer   \n",
       "44442  25041    Men       Footwear  Flip Flops  Flip Flops        Red  Summer   \n",
       "44443  25042    Men        Apparel     Topwear     Tshirts       Blue    Fall   \n",
       "44445  25043  Women    Accessories     Watches     Watches       Pink  Winter   \n",
       "\n",
       "         year   usage                               productDisplayName  \\\n",
       "2      2016.0  Casual                         Titan Women Silver Watch   \n",
       "4      2012.0  Casual                            Puma Men Grey T-shirt   \n",
       "5      2011.0  Casual             Inkfruit Mens Chain Reaction T-shirt   \n",
       "9      2016.0  Casual                           Skagen Men Black Watch   \n",
       "11     2012.0  Casual           Fossil Women Black Huarache Weave Belt   \n",
       "...       ...     ...                                              ...   \n",
       "44439  2011.0  Casual               Tantra Women Printed Peach T-shirt   \n",
       "44440  2012.0  Casual                     Sepia Women Blue Printed Top   \n",
       "44442  2011.0  Casual               Lotto Men's Soccer Track Flip Flop   \n",
       "44443  2011.0  Casual             Puma Men Graphic Stellar Blue Tshirt   \n",
       "44445  2016.0  Casual  Fossil Women Pink Dial Chronograph Watch ES3050   \n",
       "\n",
       "      productDisplayName2   filename  my_id  class_id  \n",
       "2                     NaN  59263.jpg  59263        58  \n",
       "4                     NaN  53759.jpg  53759        55  \n",
       "5                     NaN   1855.jpg   1855        55  \n",
       "9                     NaN  30039.jpg  30039        58  \n",
       "11                    NaN  48123.jpg  48123         6  \n",
       "...                   ...        ...    ...       ...  \n",
       "44439                 NaN  12544.jpg  12544        55  \n",
       "44440                 NaN  42234.jpg  42234        53  \n",
       "44442                 NaN   6461.jpg   6461        20  \n",
       "44443                 NaN  18842.jpg  18842        55  \n",
       "44445                 NaN  51623.jpg  51623        58  \n",
       "\n",
       "[25044 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background.df\n",
    "# evaluation.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Accessory Gift Set', 'Baby Dolls', 'Backpacks', 'Bangle',\n",
       "       'Basketballs', 'Bath Robe', 'Belts', 'Booties', 'Boxers', 'Bra',\n",
       "       'Camisoles', 'Capris', 'Compact', 'Concealer', 'Cufflinks',\n",
       "       'Deodorant', 'Dresses', 'Duffel Bag', 'Dupatta', 'Flats',\n",
       "       'Flip Flops', 'Formal Shoes', 'Gloves', 'Hair Colour', 'Handbags',\n",
       "       'Heels', 'Jackets', 'Jeggings', 'Kajal and Eyeliner', 'Kurtis',\n",
       "       'Laptop Bag', 'Leggings', 'Lip Gloss', 'Lip Liner', 'Mobile Pouch',\n",
       "       'Mufflers', 'Nail Polish', 'Necklace and Chains', 'Nightdress',\n",
       "       'Patiala', 'Pendant', 'Rain Jacket', 'Ring', 'Rompers',\n",
       "       'Rucksacks', 'Sandals', 'Shorts', 'Skirts', 'Sports Sandals',\n",
       "       'Stockings', 'Suspenders', 'Swimwear', 'Ties', 'Tops', 'Trousers',\n",
       "       'Tshirts', 'Waist Pouch', 'Waistcoat', 'Watches', 'Water Bottle'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bracelet', 'Briefs', 'Caps', 'Casual Shoes', 'Churidar',\n",
       "       'Clutches', 'Earrings', 'Eyeshadow', 'Face Moisturisers',\n",
       "       'Face Wash and Cleanser', 'Foundation and Primer',\n",
       "       'Fragrance Gift Set', 'Free Gifts', 'Highlighter and Blush',\n",
       "       'Innerwear Vests', 'Jeans', 'Jewellery Set', 'Jumpsuit',\n",
       "       'Kurta Sets', 'Kurtas', 'Lip Care', 'Lipstick', 'Lounge Pants',\n",
       "       'Lounge Shorts', 'Mascara', 'Mask and Peel', 'Messenger Bag',\n",
       "       'Night suits', 'Perfume and Body Mist', 'Salwar', 'Sarees',\n",
       "       'Scarves', 'Shirts', 'Shoe Accessories', 'Socks', 'Sports Shoes',\n",
       "       'Stoles', 'Sunglasses', 'Sunscreen', 'Sweaters', 'Sweatshirts',\n",
       "       'Track Pants', 'Tracksuits', 'Travel Accessory', 'Trunk', 'Tunics',\n",
       "       'Wallets'], dtype='<U22')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `pytest` in the root directory to run tests. There are tests specifically designed to test the integration with `few_shot.core.NShotTaskSampler` in `tests/test_few_shot_integration`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I focused on running experiments with Prototypical Networks.\n",
    "Run e.g  the following for 2- and 5-shot, 2-, 5- and 15-way experiments on the small dataset:\n",
    "\n",
    "```\n",
    "python -m experiments.few_shot_learning --dataset fashion --k-test 2 --n-test 1 --k-train 10 --n-train 1 --q-train 5 --small-dataset\n",
    "python -m experiments.few_shot_learning --dataset fashion --k-test 5 --n-test 1 --k-train 30 --n-train 1 --q-train 5 --small-dataset\n",
    "python -m experiments.few_shot_learning --dataset fashion --k-test 15 --n-test 1 --k-train 30 --n-train 1 --q-train 5 --small-dataset\n",
    "python -m experiments.few_shot_learning --dataset fashion --k-test 2 --n-test 5 --k-train 10 --n-train 5 --q-train 5 --small-dataset\n",
    "python -m experiments.few_shot_learning --dataset fashion --k-test 5 --n-test 5 --k-train 30 --n-train 5 --q-train 5 --small-dataset\n",
    "python -m experiments.few_shot_learning --dataset fashion --k-test 15 --n-test 5 --k-train 30 --n-train 5 --q-train 5 --small-dataset\n",
    "```\n",
    "\n",
    "The full set of experiments that I ran can be found in `experiments/few_shot_experiments`.\n",
    "\n",
    "For a list of arguments, refer to the help message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: few_shot_experiment.py [-h] [--dataset DATASET] [--distance DISTANCE]\n",
      "                              [--n-train N_TRAIN] [--n-test N_TEST]\n",
      "                              [--k-train K_TRAIN] [--k-test K_TEST]\n",
      "                              [--q-train Q_TRAIN] [--q-test Q_TEST]\n",
      "                              [--small-dataset] [-a ARCHITECTURE]\n",
      "                              [--pretrained] [--validate]\n",
      "                              [--n_val_classes N_VAL_CLASSES] [--gpu GPU]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --dataset DATASET\n",
      "  --distance DISTANCE\n",
      "  --n-train N_TRAIN\n",
      "  --n-test N_TEST\n",
      "  --k-train K_TRAIN\n",
      "  --k-test K_TEST\n",
      "  --q-train Q_TRAIN\n",
      "  --q-test Q_TEST\n",
      "  --small-dataset\n",
      "  -a ARCHITECTURE, --arch ARCHITECTURE\n",
      "                        model architecture: alexnet | densenet121 |\n",
      "                        densenet161 | densenet169 | densenet201 | googlenet |\n",
      "                        inception_v3 | mobilenet_v2 | resnet101 | resnet152 |\n",
      "                        resnet18 | resnet34 | resnet50 | resnext101_32x8d |\n",
      "                        resnext50_32x4d | shufflenet_v2_x0_5 |\n",
      "                        shufflenet_v2_x1_0 | shufflenet_v2_x1_5 |\n",
      "                        shufflenet_v2_x2_0 | squeezenet1_0 | squeezenet1_1 |\n",
      "                        vgg11 | vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn\n",
      "                        | vgg19 | vgg19_bn (default: resnet18)\n",
      "  --pretrained\n",
      "  --validate\n",
      "  --n_val_classes N_VAL_CLASSES\n",
      "  --gpu GPU\n"
     ]
    }
   ],
   "source": [
    "run \"~/few-shot-learning/experiments/few_shot_experiment.py\" -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the only available option `--dataset fashion`. Arguments `--n-train N, --n-test N, --k-train K, --k-test K` determine to the type of few-shot learning to run, that is the number of support samples and the number of classes to encounter during training and testing, respectively. Arguments `--q-train Q, --q-test` determine the number of query examples and can be flexibly chosen. It's important though to ensure `n_train + q_train <= 10` and `n_test+ q_test <= 10` as this is the minimum amount of samples per class in both the background set and the evaluation set for the the fashion dataset.\n",
    "\n",
    "Usually, one would chose `n_train=n_test` and `k_train=k_test` to match training and testing conditions exactly. However, it is possible to chose `k_train > k_test` to make the training conditions harder, possibly combating overfitting. I used this strategy in all experiments as suggested in the proto-nets paper.\n",
    "\n",
    "The number of query samples is kept at `q_train=5` and `q_test=1`.\n",
    "\n",
    "Furthermore arguments include `--small-dataset` to train on the smaller image sizes (80-by-60 pixels) which should be chosen as long as `--pretrained` is not used. The default model is a 4-layer conv-net, such that for the given image size, the embedding dimension is reasonable. In the original code, this model was used for 28-by-28 Omniglot images and 84-by-84 miniImageNet images, which is the same ballpark as the small fashion dataset.\n",
    "\n",
    "If `--small-dataset` is not given, the images will have size 400-by-300 as in the transfer learning experimets. This is too large for a 4-layer conv-net, as the embedding dimension will explode together with the memory and computation requirements. Instead, I used `--pretrained` to use a pre-defined model from the ImageNet model zoo. For all models from the zoo, the last fully connected layer will be discarded (actually replaced with an identity), so that the models can serve as embedding models. I only ran experiments with `--arch resnet18` which is also the default. That turned out to be intensive enough, since for few-shot learning with Prototypical Networks, the batch size is `(n_train + q_train) * k_train`. This goes up to 300 for the most difficult configuration I had in mind (`n_train=5, q_train=5, k_train=30`), which I found didn't fit on a single 12GB GPU. One could think about multi-GPU training but I am not certain at the moment whether this can work as straightforwardly as with transfer learning.\n",
    "\n",
    "Now, technically, it is only important to use the larger architecture, and not the pretrained weights for the 400-by-300 pixel images. However, using pretrained weights was one the ideas I had of how to improve the performance, and so I blended model architecture and pretraining in the arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy to test model and improve performance\n",
    "\n",
    "The important decisions I made for training can be summarized like this:\n",
    "\n",
    "- As in the original implementation:\n",
    "    - *n_epochs*: 80\n",
    "    - *learning rate schedule*: `learning_rate=1e-3`, halfed every `drop_lr_every=20` epochs\n",
    "    - *episodes_per_epoch*: 100\n",
    "    - *model architecture*: 4-layer conv-net (for 80-by-60 pixel small dataset) \n",
    "    - *distance*: L2\n",
    "- Changes:\n",
    "    - *validation*: introduced the possibility to monitor validation accuracy. The validation set is constructed by taking a subset of the background classes. The size of the subset is determined by `n_val_classes` and should be at least `k_test`. The original implementation monitored only performance on the evaluation/meta-test set. This is not a good idea as it cannot be used for early-stopping or best model selection.\n",
    "    - *validation_episodes*: 200\n",
    "    - *evaluation_episodes*: 1000 (the original code had bug and evaluated only on *episodes_per_epoch* episodes)\n",
    "    - *model architecture*: **resnet18** with pretrained weights (for 400-by-300 pixel dataset)\n",
    "    - *data augmentation*: `torchvision.transforms.RandomResizedCrop`, `torchvision.transforms.RandomPerspective`, `torchvision.transforms.RandomHorizontalFlip` only for the background/meta-training set.\n",
    "    - *n-shot, k-way*:\n",
    "        - 1-shot, 2-way: `n_train=1, n_test=1, k_train=10, k_test=2, q_train=5, q_test=1`\n",
    "        - 1-shot, 5-way: `n_train=1, n_test=1, k_train=30, k_test=5, q_train=5, q_test=1`\n",
    "        - 1-shot, 15-way: `n_train=1, n_test=1, k_train=30, k_test=15, q_train=5, q_test=1`\n",
    "        - 5-shot, 2-way: `n_train=5, n_test=5, k_train=10, k_test=2, q_train=5, q_test=1`\n",
    "        - 5-shot, 5-way: `n_train=5, n_test=5, k_train=30, k_test=2, q_train=5, q_test=1`\n",
    "        - 5-shot, 15-way: `n_train=5, n_test=5, k_train=30, k_test=2, q_train=5, q_test=1`\n",
    "        - 5-shot, 15-way: `n_train=5, n_test=5, k_train=20, k_test=2, q_train=5, q_test=1` (for 400-by-300 pixel imaes, not enough memory with `k_train=30`)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.expanduser(\"~/few-shot-learning/logs/proto_nets\")\n",
    "MODEL_DIR = os.path.expanduser(\"~/few-shot-learning/models/proto_nets\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=1_kt=10_qt=5_nv=1_kv=2_qv=1_small=True_pretrained=False_validate=False.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=1_kt=30_qt=5_nv=1_kv=5_qv=1_small=True_pretrained=False_validate=False.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=1_kt=30_qt=5_nv=1_kv=15_qv=1_small=True_pretrained=False_validate=False.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=5_kt=10_qt=5_nv=5_kv=2_qv=1_small=True_pretrained=False_validate=False.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=5_kt=30_qt=5_nv=5_kv=5_qv=1_small=True_pretrained=False_validate=False.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=5_kt=30_qt=5_nv=5_kv=15_qv=1_small=True_pretrained=False_validate=False.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=1_kt=10_qt=5_nv=1_kv=2_qv=1_small=True_pretrained=False_validate=True.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=1_kt=30_qt=5_nv=1_kv=5_qv=1_small=True_pretrained=False_validate=True.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=1_kt=30_qt=5_nv=1_kv=15_qv=1_small=True_pretrained=False_validate=True.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=5_kt=10_qt=5_nv=5_kv=2_qv=1_small=True_pretrained=False_validate=True.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=5_kt=30_qt=5_nv=5_kv=5_qv=1_small=True_pretrained=False_validate=True.pth\n",
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=5_kt=30_qt=5_nv=5_kv=15_qv=1_small=True_pretrained=False_validate=True.pth\n"
     ]
    }
   ],
   "source": [
    "# load best model state_dict\n",
    "small = True\n",
    "pretrained = False\n",
    "\n",
    "validate = [False, True]\n",
    "shot = [(1,1), (5,5)]\n",
    "way = [(2,10), (5,30), (15,30)]\n",
    "query = [(1,5), (1,5)]\n",
    "\n",
    "best_model_state_dict = {}\n",
    "results = {}\n",
    "\n",
    "for val in validate:\n",
    "    \n",
    "    best_model_state_dict[val] = {}\n",
    "    results[val] = {}\n",
    "    \n",
    "    for (n_test, n_train), (q_test, q_train) in zip(shot, query):\n",
    "        \n",
    "        best_model_state_dict[val][n_test] = {}\n",
    "        results[val][n_test] = {}\n",
    "        \n",
    "        for (k_test, k_train) in way:\n",
    "            \n",
    "            param_str = f'fashion_nt={n_train}_kt={k_train}_qt={q_train}_' \\\n",
    "            f'nv={n_test}_kv={k_test}_qv={q_test}_small={small}_' \\\n",
    "            f'pretrained={pretrained}_validate={val}'\n",
    "            \n",
    "            logfile = os.path.join(LOG_DIR, param_str + \".csv\")\n",
    "            modelfile = os.path.join(MODEL_DIR, param_str + \".pth\")\n",
    "            \n",
    "            print(modelfile)\n",
    "            if not os.path.isfile(modelfile):\n",
    "                print(\"not found\")\n",
    "                continue\n",
    "                \n",
    "            state_dict = torch.load(modelfile, map_location=device)\n",
    "            best_model_state_dict[val][n_test][k_test] = state_dict\n",
    "            \n",
    "            totals = evaluate_few_shot(\n",
    "                state_dict,\n",
    "                n_shot=n_test,\n",
    "                k_way=k_test,\n",
    "                q_queries=q_test,\n",
    "                device=device,\n",
    "                architecture='resnet18',\n",
    "                pretrained=pretrained,\n",
    "                small_dataset=small,\n",
    "                metric_name=\"accuracy\",\n",
    "                evaluation_episodes=1000,\n",
    "                num_input_channels=3,\n",
    "                distance='l2'  \n",
    "            )\n",
    "            \n",
    "            results[val][n_test][k_test] = totals\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: {1: {2: {'loss': 0.2830387688097544, 'accuracy': 0.8905},\n",
       "   5: {'loss': 0.8062824005491566, 'accuracy': 0.7478},\n",
       "   15: {'loss': 1.4794104229211806, 'accuracy': 0.5820666666666666}},\n",
       "  5: {2: {'loss': 0.11855049550533295, 'accuracy': 0.9575},\n",
       "   5: {'loss': 0.3890641279128595, 'accuracy': 0.8796},\n",
       "   15: {'loss': 0.8682025781907141, 'accuracy': 0.7622666666666666}}},\n",
       " True: {1: {2: {'loss': 0.33640861096978186, 'accuracy': 0.885},\n",
       "   5: {'loss': 0.8115123350869399, 'accuracy': 0.7266},\n",
       "   15: {'loss': 1.7122219169139863, 'accuracy': 0.5309333333333334}},\n",
       "  5: {2: {'loss': 0.11383653211593628, 'accuracy': 0.962},\n",
       "   5: {'loss': 0.4309825728155229, 'accuracy': 0.8732},\n",
       "   15: {'loss': 1.0273922931402921, 'accuracy': 0.7261333333333333}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-1 accuracy for the few-shot experiments is shown in the following table:\n",
    "\n",
    "|                           | Fashion Small |     |      |      |     |      |\n",
    "|---------------------------|---------------|-----|------|------|-----|------|\n",
    "| **k-way**                 | **2**         |**5**|**15**|**2** |**5**|**15**|\n",
    "| **n-shot**                | **1**         |**1**|**1** |**5** |**5**|**5** |\n",
    "| 80 epochs                 | 89.1          |74.8 |58.2  |95.8  |88.0 |76.2  |\n",
    "| best model (validation)   | 88.5          |72.7 |53.1  |96.2  |87.3 |72.6  |\n",
    "\n",
    "It seems that the best model selection via validation accuracy underperforms the simple approach of training the model for 80 epochs. It could be that the gains of potentially combating overfitting through validation do not even out the costs of sacrificing 10-20 of the 60 available background classes for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/few-shot-learning/models/proto_nets/fashion_nt=5_kt=20_qt=5_nv=5_kv=15_qv=1_small=False_pretrained=True_validate=True.pth\n",
      "not found\n"
     ]
    }
   ],
   "source": [
    "# load best model state_dict\n",
    "small = False\n",
    "pretrained = True\n",
    "\n",
    "validate = [True] #[False, True]\n",
    "# n-shot, k-way and q-query have more complicated structure here.\n",
    "# Due to memory problems, some combinations were not possible.\n",
    "# Since concerns the training values only though.\n",
    "shot_way_query = [\n",
    "    (1,1,2,10,1,5),(1,1,5,30,1,5),(1,1,15,30,1,5),\n",
    "    (5,5,2,10,1,5),(5,5,5,20,1,5),(5,5,15,20,1,5)\n",
    "]\n",
    "\n",
    "#best_model_state_dict = {}\n",
    "#results_full = {}\n",
    "\n",
    "for val in validate:\n",
    "    \n",
    "    #best_model_state_dict[val] = {}\n",
    "    #results_full[val] = {}\n",
    "    \n",
    "    for (n_test, n_train, k_test, k_train, q_test, q_train) in shot_way_query:\n",
    "        \n",
    "        best_model_state_dict[val][(n_test, k_test)] = {}\n",
    "        results_full[val][(n_test, k_test)] = {}\n",
    "        \n",
    "        param_str = f'fashion_nt={n_train}_kt={k_train}_qt={q_train}_' \\\n",
    "        f'nv={n_test}_kv={k_test}_qv={q_test}_small={small}_' \\\n",
    "        f'pretrained={pretrained}_validate={val}'\n",
    "\n",
    "        logfile = os.path.join(LOG_DIR, param_str + \".csv\")\n",
    "        modelfile = os.path.join(MODEL_DIR, param_str + \".pth\")\n",
    "\n",
    "        print(modelfile)\n",
    "        if not os.path.isfile(modelfile):\n",
    "            print(\"not found\")\n",
    "            continue\n",
    "\n",
    "        state_dict = torch.load(modelfile, map_location=device)\n",
    "        best_model_state_dict[val][(n_test, k_test)] = state_dict\n",
    "\n",
    "        totals = evaluate_few_shot(\n",
    "            state_dict,\n",
    "            n_shot=n_test,\n",
    "            k_way=k_test,\n",
    "            q_queries=q_test,\n",
    "            device=device,\n",
    "            architecture='resnet18',\n",
    "            pretrained=pretrained,\n",
    "            small_dataset=small,\n",
    "            metric_name=\"accuracy\",\n",
    "            evaluation_episodes=400,\n",
    "            num_input_channels=3,\n",
    "            distance='l2'  \n",
    "        )\n",
    "\n",
    "        results_full[val][(n_test, k_test)] = totals\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: {(1, 2): {'loss': 0.29882320020347836, 'accuracy': 0.90625},\n",
       "  (1, 5): {'loss': 0.7771180894847567, 'accuracy': 0.783},\n",
       "  (1, 15): {'loss': 1.6507120440900325, 'accuracy': 0.5725},\n",
       "  (5, 2): {'loss': 0.12285698566585779, 'accuracy': 0.95875},\n",
       "  (5, 5): {'loss': 0.44557703804807036, 'accuracy': 0.8815},\n",
       "  (5, 15): {'loss': 0.8342426947876811, 'accuracy': 0.7483333333333333}},\n",
       " True: {(1, 2): {'loss': 0.35847300887107847, 'accuracy': 0.89625},\n",
       "  (1, 5): {'loss': 0.8223146000178531, 'accuracy': 0.757},\n",
       "  (1, 15): {'loss': 1.733450947329402, 'accuracy': 0.5735},\n",
       "  (5, 2): {'loss': 0.12618330255150795, 'accuracy': 0.96875},\n",
       "  (5, 5): {'loss': 0.3712885344750248, 'accuracy': 0.874},\n",
       "  (5, 15): {}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-1 accuracy for the dataset with 400-by-300 pixel images is shown in the table below:\n",
    "\n",
    "|                           | Fashion Small |     |      |      |     |      |\n",
    "|---------------------------|---------------|-----|------|------|-----|------|\n",
    "| **k-way**                 | **2**         |**5**|**15**|**2** |**5**|**15**|\n",
    "| **n-shot**                | **1**         |**1**|**1** |**5** |**5**|**5** |\n",
    "| 80 epochs                 | 90.6          |78.3 |57.2  |95.9  |88.1 |74.8  |\n",
    "| best model (validation)   | 89.6          |75.7 |57.3  |96.9  |87.4 |tbd   |\n",
    "\n",
    "There are a few gains here and there but overall the results with a pretrained network do not improve upon the results from before, where the images and the network are smaller. On the other hand, this shows that the approach is viable for larger image sizes, where the smaller network produces embeddings with too high a dimensionality. Unfortunately, even in this case the validation approach underperforms slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Outlook\n",
    "\n",
    "### Extensions:\n",
    "\n",
    "- *Class augmentation*: As in the Omniglot experiments in the [paper](https://arxiv.org/pdf/1703.05175v2.pdf), it could be a viable approach to augment the set of background classes. In Omniglot, this was done by 90 degree rotations of the original images, increasing the number of classes from 1200 to 4800. Note that this is different from the simple data augmentations used here, which increase the virtual amount of samples available for each class. An increase of classes could be beneficial especially to improve the monitoring of validation accuracy, which came with substantial sacrifices in the current setup with only 60 background classes.\n",
    "    It is unclear to me, however, whether rotations of the original images for the fashion data are a good way to create new virtual classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
