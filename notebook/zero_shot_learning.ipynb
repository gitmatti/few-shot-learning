{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "from few_shot_learning.datasets import FashionProductImages, FashionProductImagesSmall\n",
    "from few_shot_learning.utils_data import prepare_class_embedding, prepare_vocab,\\\n",
    "    prepare_vocab_embedding, prepare_word_embedding\n",
    "from few_shot_learning.train_zero_shot import zero_shot_training\n",
    "# from few_shot_learning.utils_evaluation import evaluate_few_shot\n",
    "from config import DATA_PATH, PATH\n",
    "from few_shot_learning.models import Identity, ClassEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Learning\n",
    "\n",
    "# 1. Introduction and Strategy\n",
    "\n",
    "The idea of zero-shot learning is to train a model to be able to classify images from unseen classes, that is classes of which the model has not seen any samples during training. An example in the context of the fashion dataset could be the task of deciding whether an new, unseen, image is a \"Jeans\" or a \"Casual Shoe\" when neither jeans nor casual shoe images were present a training time. At training time, the model can only access images and class labels from a set of training classes, i.e. classes disjoint from the set of unseen classes. This could be images of \"Formal Shoes\" and \"Tshirts\".\n",
    "\n",
    "The task of zero-shot learning is different from even one-shot learning, since in few-shot learning the model at test time has access to a few sample images from all unseen classes and can adapt using these samples before being asked to classify unseen images.\n",
    "\n",
    "To be able to classify unseen classes with zero-shot learning, the model needs to make use of at least one of two strategies. Either there is additional class-level meta information that describes seen and unseen classes in terms of additional attributes, e.g. visual attributes. In this case the model can classify unseen images of unseen classes at test time by relating the unseen classes to classes seen at training time via their attributes. \n",
    "Or the model can use the class labels themselves to understand what to look for in an unseen image by relating them to classes seen at training time semantically. In the context of the fashion dataset, the example above already illustrates this strategy: If the model has knowledge of the semantic similarity between \"Formal Shoes\" (training class) and \"Casual Shoes\" (unseen class), it could reasonably deduce that an unseen image of a casual shoe must look more like a formal shoe than like jeans.\n",
    "\n",
    "For the fashion dataset, the only class-level meta information are the `masterCategory` and `subCategory` columns of the data. The column `productDisplayName`, although semantically informative, represents sample-level information and can thus not be used for zero-shot learning. Or rather, it can not straightforwardly be used for zero-shot learning. One can imagine pooling the semantic descriptions of all samples of a given class, distilling them into a a single class-level description. This could be, for example, a measure of how often certain descriptors like \"green\" are used to describe images of the class. Importantly, if this strategy were used, the product descriptions of indivdual images could not be used.\n",
    "\n",
    "Here, I chose a different strategy, combining the class-level attributes `masterCategory` and `subCategory` as categorical features with NLP features of the class labels through pre-trained semantic word embeddings. Specifically, I chose to represent class labels as vectors via the [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) language model. The algorithm used for zero-shot learning with these features were **Prototypical Networks** in the zero-shot configuration as described in the paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Methods and Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data with class attribute configuration\n",
    "all_data = FashionProductImagesSmall(split='all', classes=None, return_class_attributes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `masterCategory` is a class-level meta information which takes 7 different values. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apparel' 'Accessories' 'Footwear' 'Personal Care' 'Free Items'\n",
      " 'Sporting Goods' 'Home']\n"
     ]
    }
   ],
   "source": [
    "print(all_data.df_meta[\"masterCategory\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to the represent these as categorical, one-hot features, ignoring their semantic meaning. The dataset can return them with `return_class_attributes=True`, e.g. via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, attr = all_data[0]\n",
    "attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of utility functions uses the word representations from **GloVe: Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip** to build word embeddings of the class labels. This is a common strategy e.g. for zero-shot learning on ImageNet (see this review [paper](https://arxiv.org/pdf/1707.00600.pdf)).\n",
    "\n",
    "A slight complication arises for the fashion data in that the class labels are sometimes multi-word descriptions, e.g. \"Perfume and Body Mist\" or \"Laptop Bag\". A possible solution would be to learn an RNN on top of the single-word embeddings, outputting a fixed size embedding vector for all classes. Given how complicated the setup is expected to be, however, it is preferable to not introduce additional parameters when it can be avoided. For multi-word class labels, I chose to simply pool all single-word embeddings for a given multi-word class label via a simple averaging operation as suggested in this [paper](https://www.aclweb.org/anthology/P18-1041.pdf).\n",
    "\n",
    "The class label embeddings thus are $300$-dimensional vectors, which are simply concatenated with the $7$-dimensional attribute vectors encoding the `masterCategory`. A simple linear layer is learned on top to project these features into a $256$-dimensional cross-modal embedding space into which the query images will be mapped as well. As in the Prototypical Networks paper, the $256$-dimensional class embeddings are normalized to unit length in the cross-modal space.\n",
    "\n",
    "To embed images in this cross-modal space, a linear layer is learned on top of pre-trained ResNet18 image features, which are $512$-dimensional. The ResNet18 features are not fine-tuned. There is a potential problem with this approach when there are overlapping classes between ImageNet (on which ResNet18 was trained) and the unseen test classes, but I chose to ignore that.\n",
    "\n",
    "Below is a code snippet showcasing the class label embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this prepares the indexing of word2vec for glove and takes a while\n",
    "prepare_word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = FashionProductImagesSmall(split='all', classes='background', return_class_attributes=True)\n",
    "# evaluation = FashionProductImagesSmall(split='all', classes='evaluation', return_class_attributes=True)\n",
    "\n",
    "target_vocab = prepare_vocab(background.df_meta, columns=[\"articleType\"])\n",
    "vocab_embedding, word2idx = prepare_vocab_embedding(target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9351e-02,  6.9610e-02, -1.3461e-01, -3.5931e-01,  3.8046e-01,\n",
       "        4.4326e-01,  7.0563e-02, -1.6641e-01,  2.3013e-01, -1.8636e-02,\n",
       "       -9.0016e-02,  3.6856e-01, -3.2842e-01,  1.3455e-01,  5.6520e-01,\n",
       "       -6.7276e-01,  3.0663e-01,  2.4319e-01, -1.6085e-01,  2.9144e-01,\n",
       "       -6.9306e-01,  1.8493e-01, -1.0965e-01, -4.1638e-01, -2.7546e-01,\n",
       "       -1.0150e-01, -2.2765e-01, -5.8273e-02,  2.2619e-02,  4.9403e-01,\n",
       "        3.3521e-01, -6.1759e-01, -1.6073e-01,  3.8577e-01, -2.6317e-01,\n",
       "       -5.3999e-01,  1.4394e-01,  4.0844e-01, -6.3808e-02,  4.8194e-01,\n",
       "       -2.4429e-01,  4.9381e-02, -4.8517e-01, -3.7395e-01, -3.0018e-01,\n",
       "       -2.8939e-01,  3.5502e-01,  1.5590e-01,  4.3168e-01,  4.3281e-02,\n",
       "        1.9047e-01,  9.2106e-02,  1.1732e+00, -6.1249e-01,  1.4235e-01,\n",
       "       -2.2451e-01,  4.2231e-01,  1.0565e-01,  5.6860e-01, -3.1866e-02,\n",
       "       -4.7184e-01, -3.7409e-01,  5.8321e-01, -1.2297e-01, -6.7348e-02,\n",
       "       -2.8081e-02, -6.1327e-01,  6.7741e-01, -2.2748e-02, -7.4274e-01,\n",
       "        4.8788e-02,  1.3788e-01,  6.4438e-01, -3.3544e-01,  6.1862e-01,\n",
       "        7.5012e-01,  1.1786e-01,  2.5781e-01,  8.4405e-02,  2.0349e-01,\n",
       "       -3.8508e-01,  1.6445e-01,  1.3646e-01,  3.1133e-02,  7.8684e-01,\n",
       "        2.2409e-01,  4.2362e-01,  5.4829e-01, -4.4495e-04, -2.6747e-01,\n",
       "        2.4741e-01, -3.0000e-01,  3.0254e-01,  2.2347e-01,  6.1531e-02,\n",
       "        1.4731e-01, -5.9372e-01, -2.8617e-01, -7.2025e-02, -5.3411e-01,\n",
       "        1.5891e-01, -2.6978e-01, -5.4990e-01,  2.5058e-01,  8.3271e-02,\n",
       "       -5.7017e-01, -4.3960e-01, -3.4635e-01, -4.1975e-01,  3.1934e-01,\n",
       "       -1.0213e-01, -2.3536e-01, -1.4529e-01,  4.4506e-02, -4.1748e-02,\n",
       "       -2.5846e-01,  4.1071e-01, -1.0848e-02, -2.0234e-01, -1.3217e-01,\n",
       "        1.5073e-01,  1.7581e-01,  5.9703e-01, -1.2587e-01, -9.5178e-02,\n",
       "        1.4615e-01,  3.7618e-02,  8.1040e-01, -2.7808e-01, -2.2755e-01,\n",
       "        3.5696e-01, -6.6224e-01, -8.5044e-01, -6.9876e-01, -3.3112e-01,\n",
       "        1.9057e-01,  4.8356e-02,  3.7997e-01, -6.2070e-04,  1.1583e-02,\n",
       "       -1.7981e+00,  7.8641e-02,  1.7304e-02,  2.2436e-01,  2.0716e-01,\n",
       "        2.2337e-02,  7.1773e-01, -1.5101e-01,  2.0815e-01,  5.2213e-02,\n",
       "        5.9687e-02, -1.9123e-01, -5.0328e-01, -3.1115e-01, -7.3341e-01,\n",
       "       -3.1045e-01, -2.7280e-01, -5.3640e-01,  9.9222e-02, -3.4742e-01,\n",
       "        1.9654e-01,  3.8701e-01,  1.2337e-01,  5.2634e-01,  1.8777e-02,\n",
       "       -6.2911e-02, -4.0637e-01,  5.5475e-01, -6.6978e-01,  4.5189e-01,\n",
       "       -3.9126e-01,  6.9797e-01,  2.2314e-01,  6.9688e-01, -4.8094e-02,\n",
       "       -4.3178e-01, -6.7088e-01,  4.5842e-01,  4.3202e-01,  8.5374e-02,\n",
       "       -3.4214e-01,  3.4154e-01,  3.1689e-01, -9.3929e-02,  2.6468e-01,\n",
       "       -3.4185e-01,  1.6679e-01, -6.2991e-02,  2.4653e-01,  5.4794e-02,\n",
       "        3.9409e-02,  2.6028e-01, -1.8306e-01, -7.2060e-01,  4.5706e-01,\n",
       "        8.9071e-02,  1.0368e-01, -1.6998e-01,  5.9886e-02, -9.2426e-02,\n",
       "        1.6962e-01,  4.6814e-03,  1.8467e-01, -1.8015e-01, -3.5862e-02,\n",
       "       -2.9767e-01,  9.8045e-02, -5.5546e-01, -8.2777e-01,  4.0777e-01,\n",
       "       -8.5967e-03,  3.1879e-01, -7.4239e-01,  2.9933e-01, -3.8823e-01,\n",
       "        1.5937e-01,  9.6753e-02, -8.2773e-02, -1.0410e+00,  5.9717e-02,\n",
       "       -5.6211e-01, -2.7769e-01,  8.9435e-02, -8.8380e-01, -3.1840e-01,\n",
       "       -2.6675e-01, -2.0417e-01,  1.1020e-01, -1.7705e-01,  1.2518e-02,\n",
       "        2.5842e-01, -8.3802e-02,  8.6465e-03,  1.8434e-03,  1.6897e-01,\n",
       "       -3.3572e-01, -9.4865e-02, -4.3497e-01,  3.2923e-01,  3.3312e-01,\n",
       "       -1.9859e-01, -2.5582e-01, -4.5290e-01, -1.3602e-01,  2.7492e-01,\n",
       "        5.7997e-01,  1.6806e-01, -3.0398e-01,  4.6917e-02,  9.4684e-02,\n",
       "       -1.1633e-01, -1.1298e-01,  2.7911e-01,  5.4431e-01, -8.0706e-02,\n",
       "       -5.7093e-01, -5.5659e-02,  6.7837e-02,  1.9249e-02, -3.7662e-02,\n",
       "        3.0177e-02,  7.9912e-01, -6.2956e-01,  3.4354e-01,  4.0641e-01,\n",
       "        7.3833e-01,  1.9483e-01,  1.5652e-01, -2.4156e-01,  2.9452e-01,\n",
       "       -1.0493e+00, -6.5963e-02,  4.4950e-01,  4.8001e-01,  2.8242e-01,\n",
       "       -2.6112e-01, -3.0561e-03,  1.4469e-02,  6.3108e-01, -4.5948e-02,\n",
       "       -2.5581e-01, -1.2590e-01, -2.4036e-01,  3.0319e-02, -2.0896e-01,\n",
       "        1.0630e-01,  2.0066e-01,  1.6771e-01, -6.3676e-01, -2.2444e-01,\n",
       "       -3.1338e-01, -2.3925e-01,  1.5883e-01,  1.3302e-01, -8.7227e-02,\n",
       "       -1.8998e-01,  3.1652e-01, -3.1553e-01,  3.6538e-01,  3.1196e-01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_embedding[word2idx[\"tshirts\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run\n",
    "\n",
    "```\n",
    "python -m experiments.zero_shot_experiment --k-train 10 --k-test 2 --q-train 10 --q-test 1 --small-dataset --pretrained --freeze\n",
    "\n",
    "python -m experiments.zero_shot_experiment --k-train 20 --k-test 5 --q-train 10 --q-test 1 --small-dataset --pretrained --freeze\n",
    "\n",
    "python -m experiments.zero_shot_experiment --k-train 30 --k-test 15 --q-train 10 --q-test 1 --small-dataset --pretrained --freeze\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.expanduser(\"~/few-shot-learning/logs/proto_nets\")\n",
    "MODEL_DIR = os.path.expanduser(\"~/few-shot-learning/models/proto_nets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = True\n",
    "pretrained = True\n",
    "\n",
    "validate = [False]\n",
    "shot_way_query = [(0,0,2,10,1,10), (0,0,5,20,1,10), (0,0,15,30,1,10)]\n",
    "\n",
    "# best_model_state_dict = {}\n",
    "csv_logs = {}\n",
    "top1_accuracy = {}\n",
    "\n",
    "for val in validate:\n",
    "    \n",
    "    # best_model_state_dict[val] = {}\n",
    "    csv_logs[val] = {}\n",
    "    top1_accuracy[val] = {}\n",
    "    \n",
    "    for (n_test, n_train, k_test, k_train, q_test, q_train) in shot_way_query:\n",
    "        \n",
    "        param_str = f'fashion_nt={n_train}_kt={k_train}_qt={q_train}_' \\\n",
    "        f'nv={n_test}_kv={k_test}_qv={q_test}_small={small}_' \\\n",
    "        f'pretrained={pretrained}_validate={val}'\n",
    "\n",
    "        logfile = os.path.join(LOG_DIR, param_str + \".csv\")\n",
    "        modelfile = os.path.join(MODEL_DIR, param_str + \".pth\")\n",
    "        \n",
    "        # best_model_state_dict[val][(n_test, k_test)] = {}\n",
    "        csv_log = pandas.read_csv(logfile)\n",
    "        csv_logs[val][(n_test, k_test)] = csv_log\n",
    "        top1_accuracy[val][(n_test, k_test)] = csv_log[f\"val_{n_test}-shot_{k_test}-way_acc\"].iloc[-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: {(0, 2): 0.77, (0, 5): 0.484, (0, 15): 0.20266666666666666}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 1 accuracy for initial zero-shot learning experiments is shown below:\n",
    "\n",
    "|                           | Fashion Small |     |      |\n",
    "|---------------------------|---------------|-----|------|\n",
    "| **k-way, zero_shot**      | **k=2**       |**k=5**|**k=15**|\n",
    "| 40 epochs                 | 77.0          |48.4 |20.2  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
