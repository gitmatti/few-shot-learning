{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.datasets.utils import check_integrity #, download_and_extract_archive\n",
    "import pandas\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import copy\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.expanduser(\"~/few-shot-learning/\"))\n",
    "from few_shot.datasets import FashionProductImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = FashionProductImages(\"~/data\", classes=\"top\", split=\"train\", transform=data_transforms[\"train\"])\n",
    "\n",
    "X, y = fashion[10]\n",
    "# X, y = fashion[10:15] # fails\n",
    "\n",
    "len(fashion)\n",
    "\n",
    "for i in range(len(fashion)):\n",
    "    X, y = fashion[i]\n",
    "    # print(y.shape)\n",
    "    # if not isinstance(y, int):\n",
    "    #    import pdb; pdb.set_trace()\n",
    "    if not (X.shape[1]==80 and X.shape[2]==60):\n",
    "        import pdb; pdb.set_trace()\n",
    "\n",
    "counter = 0\n",
    "for (i, batch) in enumerate(train_loader):\n",
    "    counter += 64\n",
    "    print(counter)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sample_count = np.bincount(fashion.target_indices, minlength=fashion.n_classes)\n",
    "print(class_sample_count)\n",
    "print(len(class_sample_count))\n",
    "print(sum(class_sample_count))\n",
    "\n",
    "print(np.unique(fashion.targets))\n",
    "print(len(np.unique(fashion.targets)))\n",
    "\n",
    "print(np.unique(fashion.df_meta[\"articleType\"]))\n",
    "print(len(np.unique(fashion.df_meta[\"articleType\"])))\n",
    "\n",
    "# all perfumes are from 2017, which means they're all in the test set\n",
    "fashion.df_meta[\n",
    "    (fashion.df_meta[\"articleType\"]==\"Perfume and Body Mist\")\n",
    "     & (fashion.df_meta[\"year\"] == 2017.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop((80,60), scale=(0.8, 1.0)),\n",
    "        # transforms.Resize((80,60)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((80,60)),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((80,60)),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    classes: {\n",
    "        split: FashionProductImages(\n",
    "            \"~/data\",\n",
    "            split='train' if split in ['train', 'val'] else 'test',\n",
    "            classes=classes,\n",
    "            transform=data_transforms[split]\n",
    "        ) for split in [\"train\", \"test\", \"val\"]\n",
    "    } for classes in [\"top\", \"bottom\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_val_sampler(trainset, train_size=0.9, balanced_training=True, stratify=True):\n",
    "    n_classes = trainset.n_classes\n",
    "    n_samples = len(trainset)\n",
    "    indices = np.arange(n_samples)\n",
    "    labels = trainset.target_indices\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(\n",
    "        indices, train_size=0.9,\n",
    "        stratify=trainset.target_indices if stratify else None\n",
    "    )\n",
    "    \n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    \n",
    "    if balanced_training:\n",
    "        class_sample_count = np.bincount(labels[train_indices], minlength=n_classes)\n",
    "        class_sample_count = torch.from_numpy(class_sample_count).float()\n",
    "\n",
    "        class_weights = torch.zeros_like(class_sample_count)\n",
    "        class_weights[class_sample_count > 0] = 1./class_sample_count[class_sample_count > 0]\n",
    "\n",
    "        train_weights = class_weights[labels[train_indices]]\n",
    "        dataset_weights = torch.zeros(n_samples)\n",
    "        dataset_weights[train_indices] = train_weights\n",
    "        \n",
    "        # TODO: in this way, train_loader will still produce n_samples samples per epoch\n",
    "        # (instead of train_size*n_samples)\n",
    "        train_sampler = WeightedRandomSampler(dataset_weights, n_samples, replacement=True)\n",
    "    else:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        \n",
    "    return train_sampler, train_indices, val_sampler, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# train_size = int(len(fashion) * 0.9)\n",
    "# trainset, valset = random_split(fashion, [train_size, len(fashion) - train_size])\n",
    "trainset = datasets['top']['train']\n",
    "valset = datasets['top']['val']\n",
    "\n",
    "train_sampler, train_indices, val_sampler, val_indices = get_train_and_val_sampler(trainset, balanced_training=True)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, num_workers=4, sampler=train_sampler)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, num_workers=4, sampler=val_sampler)\n",
    "\n",
    "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "# dataset_sizes = {\"train\": len(train_indices), \"val\": len(val_indices)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 947.  977.  948.  967.  957.  928.  908.  929.  878.  971.    0.  943.\n",
      " 1007. 1000.  932.  972.  965.  897.  968.  906.]\n",
      "[0.05261111 0.05427778 0.05266667 0.05372222 0.05316667 0.05155556\n",
      " 0.05044444 0.05161111 0.04877778 0.05394444 0.         0.05238889\n",
      " 0.05594444 0.05555556 0.05177778 0.054      0.05361111 0.04983333\n",
      " 0.05377778 0.05033333]\n",
      "18000.0\n",
      "[ 25.  49.  66. 155.  51.  36. 101.  77.  34. 114.   0.  48. 113.  50.\n",
      "  88. 104. 106. 275.  60. 248.]\n",
      "[0.01388889 0.02722222 0.03666667 0.08611111 0.02833333 0.02\n",
      " 0.05611111 0.04277778 0.01888889 0.06333333 0.         0.02666667\n",
      " 0.06277778 0.02777778 0.04888889 0.05777778 0.05888889 0.15277778\n",
      " 0.03333333 0.13777778]\n",
      "1800.0\n"
     ]
    }
   ],
   "source": [
    "y_counts = np.zeros(trainset.n_classes)\n",
    "\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    y_counts += np.bincount(y, minlength=20)\n",
    "    \n",
    "print(y_counts)\n",
    "print(y_counts / y_counts.sum())\n",
    "print(y_counts.sum())\n",
    "\n",
    "y_counts = np.zeros(valset.n_classes)\n",
    "\n",
    "for batch in val_loader:\n",
    "    X, y = batch\n",
    "    y_counts += np.bincount(y, minlength=20)\n",
    "    \n",
    "print(y_counts)\n",
    "print(y_counts / y_counts.sum())\n",
    "print(y_counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    dataset_sizes = {x: len(dataloaders[x].sampler) for x in ['train','val']}\n",
    "    \n",
    "    n_classes = dataloaders['train'].dataset.n_classes\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            # TODO.refactor\n",
    "            running_confusion_matrix = np.zeros([model.fc.out_features]*2)\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                # multiply batch loss with batch_size to go from mean to sum\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_confusion_matrix += confusion_matrix(labels.cpu(), preds.cpu(), labels=np.arange(n_classes))\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_conf_matrix = running_confusion_matrix.copy()\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_conf_matrix = epoch_conf_matrix.copy()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, best_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "# model_ft = models.resnet50(pretrained=True)\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Alternatively: nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, trainset.n_classes)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train Loss: 0.5853 Acc: 0.8067\n",
      "val Loss: 0.4175 Acc: 0.8522\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "train Loss: 0.3727 Acc: 0.8752\n",
      "val Loss: 0.3861 Acc: 0.8594\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "train Loss: 0.3087 Acc: 0.8944\n",
      "val Loss: 0.4481 Acc: 0.8467\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "train Loss: 0.2632 Acc: 0.9091\n",
      "val Loss: 0.3006 Acc: 0.8889\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "train Loss: 0.2610 Acc: 0.9090\n",
      "val Loss: 0.3676 Acc: 0.8706\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "train Loss: 0.1890 Acc: 0.9338\n",
      "val Loss: 0.2332 Acc: 0.9156\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "train Loss: 0.1787 Acc: 0.9378\n",
      "val Loss: 0.2316 Acc: 0.9167\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "train Loss: 0.1593 Acc: 0.9424\n",
      "val Loss: 0.2203 Acc: 0.9200\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "train Loss: 0.1556 Acc: 0.9433\n",
      "val Loss: 0.2379 Acc: 0.9200\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "train Loss: 0.1570 Acc: 0.9441\n",
      "val Loss: 0.2496 Acc: 0.9178\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9547\n",
      "val Loss: 0.2025 Acc: 0.9244\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "train Loss: 0.1178 Acc: 0.9569\n",
      "val Loss: 0.2172 Acc: 0.9272\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "train Loss: 0.1157 Acc: 0.9573\n",
      "val Loss: 0.1908 Acc: 0.9339\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "train Loss: 0.1042 Acc: 0.9609\n",
      "val Loss: 0.2163 Acc: 0.9244\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "train Loss: 0.1030 Acc: 0.9626\n",
      "val Loss: 0.2149 Acc: 0.9278\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "train Loss: 0.0902 Acc: 0.9678\n",
      "val Loss: 0.1969 Acc: 0.9367\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "train Loss: 0.0790 Acc: 0.9713\n",
      "val Loss: 0.2064 Acc: 0.9356\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9722\n",
      "val Loss: 0.2045 Acc: 0.9322\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "train Loss: 0.0763 Acc: 0.9724\n",
      "val Loss: 0.2243 Acc: 0.9272\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "train Loss: 0.0670 Acc: 0.9757\n",
      "val Loss: 0.2113 Acc: 0.9333\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n",
      "train Loss: 0.0645 Acc: 0.9764\n",
      "val Loss: 0.2149 Acc: 0.9361\n",
      "\n",
      "Epoch 22/100\n",
      "----------\n",
      "train Loss: 0.0580 Acc: 0.9793\n",
      "val Loss: 0.2208 Acc: 0.9333\n",
      "\n",
      "Epoch 23/100\n",
      "----------\n",
      "train Loss: 0.0591 Acc: 0.9783\n",
      "val Loss: 0.2213 Acc: 0.9361\n",
      "\n",
      "Epoch 24/100\n",
      "----------\n",
      "train Loss: 0.0563 Acc: 0.9792\n",
      "val Loss: 0.2024 Acc: 0.9406\n",
      "\n",
      "Epoch 25/100\n",
      "----------\n",
      "train Loss: 0.0585 Acc: 0.9781\n",
      "val Loss: 0.2055 Acc: 0.9361\n",
      "\n",
      "Epoch 26/100\n",
      "----------\n",
      "train Loss: 0.0517 Acc: 0.9813\n",
      "val Loss: 0.2067 Acc: 0.9367\n",
      "\n",
      "Epoch 27/100\n",
      "----------\n",
      "train Loss: 0.0460 Acc: 0.9837\n",
      "val Loss: 0.2138 Acc: 0.9372\n",
      "\n",
      "Epoch 28/100\n",
      "----------\n",
      "train Loss: 0.0497 Acc: 0.9824\n",
      "val Loss: 0.2116 Acc: 0.9400\n",
      "\n",
      "Epoch 29/100\n",
      "----------\n",
      "train Loss: 0.0440 Acc: 0.9837\n",
      "val Loss: 0.2215 Acc: 0.9356\n",
      "\n",
      "Epoch 30/100\n",
      "----------\n",
      "train Loss: 0.0468 Acc: 0.9832\n",
      "val Loss: 0.2138 Acc: 0.9406\n",
      "\n",
      "Epoch 31/100\n",
      "----------\n",
      "train Loss: 0.0444 Acc: 0.9839\n",
      "val Loss: 0.2123 Acc: 0.9383\n",
      "\n",
      "Epoch 32/100\n",
      "----------\n",
      "train Loss: 0.0418 Acc: 0.9852\n",
      "val Loss: 0.2222 Acc: 0.9383\n",
      "\n",
      "Epoch 33/100\n",
      "----------\n",
      "train Loss: 0.0425 Acc: 0.9844\n",
      "val Loss: 0.2242 Acc: 0.9383\n",
      "\n",
      "Epoch 34/100\n",
      "----------\n",
      "train Loss: 0.0452 Acc: 0.9831\n",
      "val Loss: 0.2219 Acc: 0.9350\n",
      "\n",
      "Epoch 35/100\n",
      "----------\n",
      "train Loss: 0.0380 Acc: 0.9861\n",
      "val Loss: 0.2237 Acc: 0.9378\n",
      "\n",
      "Epoch 36/100\n",
      "----------\n",
      "train Loss: 0.0390 Acc: 0.9867\n",
      "val Loss: 0.2203 Acc: 0.9389\n",
      "\n",
      "Epoch 37/100\n",
      "----------\n",
      "train Loss: 0.0402 Acc: 0.9864\n",
      "val Loss: 0.2249 Acc: 0.9372\n",
      "\n",
      "Epoch 38/100\n",
      "----------\n",
      "train Loss: 0.0436 Acc: 0.9846\n",
      "val Loss: 0.2193 Acc: 0.9394\n",
      "\n",
      "Epoch 39/100\n",
      "----------\n",
      "train Loss: 0.0444 Acc: 0.9838\n",
      "val Loss: 0.2224 Acc: 0.9389\n",
      "\n",
      "Epoch 40/100\n",
      "----------\n",
      "train Loss: 0.0401 Acc: 0.9859\n",
      "val Loss: 0.2189 Acc: 0.9389\n",
      "\n",
      "Epoch 41/100\n",
      "----------\n",
      "train Loss: 0.0395 Acc: 0.9859\n",
      "val Loss: 0.2187 Acc: 0.9400\n",
      "\n",
      "Epoch 42/100\n",
      "----------\n",
      "train Loss: 0.0358 Acc: 0.9872\n",
      "val Loss: 0.2218 Acc: 0.9383\n",
      "\n",
      "Epoch 43/100\n",
      "----------\n",
      "train Loss: 0.0374 Acc: 0.9849\n",
      "val Loss: 0.2211 Acc: 0.9394\n",
      "\n",
      "Epoch 44/100\n",
      "----------\n",
      "train Loss: 0.0393 Acc: 0.9864\n",
      "val Loss: 0.2223 Acc: 0.9400\n",
      "\n",
      "Epoch 45/100\n",
      "----------\n",
      "train Loss: 0.0362 Acc: 0.9867\n",
      "val Loss: 0.2211 Acc: 0.9394\n",
      "\n",
      "Epoch 46/100\n",
      "----------\n",
      "train Loss: 0.0359 Acc: 0.9873\n",
      "val Loss: 0.2183 Acc: 0.9394\n",
      "\n",
      "Epoch 47/100\n",
      "----------\n",
      "train Loss: 0.0362 Acc: 0.9869\n",
      "val Loss: 0.2212 Acc: 0.9394\n",
      "\n",
      "Epoch 48/100\n",
      "----------\n",
      "train Loss: 0.0384 Acc: 0.9866\n",
      "val Loss: 0.2193 Acc: 0.9400\n",
      "\n",
      "Epoch 49/100\n",
      "----------\n",
      "train Loss: 0.0352 Acc: 0.9879\n",
      "val Loss: 0.2193 Acc: 0.9383\n",
      "\n",
      "Epoch 50/100\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 0.9875\n",
      "val Loss: 0.2199 Acc: 0.9400\n",
      "\n",
      "Epoch 51/100\n",
      "----------\n",
      "train Loss: 0.0372 Acc: 0.9871\n",
      "val Loss: 0.2211 Acc: 0.9411\n",
      "\n",
      "Epoch 52/100\n",
      "----------\n",
      "train Loss: 0.0364 Acc: 0.9872\n",
      "val Loss: 0.2212 Acc: 0.9406\n",
      "\n",
      "Epoch 53/100\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 0.9866\n",
      "val Loss: 0.2193 Acc: 0.9406\n",
      "\n",
      "Epoch 54/100\n",
      "----------\n",
      "train Loss: 0.0369 Acc: 0.9876\n",
      "val Loss: 0.2196 Acc: 0.9383\n",
      "\n",
      "Epoch 55/100\n",
      "----------\n",
      "train Loss: 0.0379 Acc: 0.9876\n",
      "val Loss: 0.2208 Acc: 0.9383\n",
      "\n",
      "Epoch 56/100\n",
      "----------\n",
      "train Loss: 0.0380 Acc: 0.9864\n",
      "val Loss: 0.2209 Acc: 0.9383\n",
      "\n",
      "Epoch 57/100\n",
      "----------\n",
      "train Loss: 0.0381 Acc: 0.9865\n",
      "val Loss: 0.2201 Acc: 0.9372\n",
      "\n",
      "Epoch 58/100\n",
      "----------\n",
      "train Loss: 0.0402 Acc: 0.9854\n",
      "val Loss: 0.2224 Acc: 0.9372\n",
      "\n",
      "Epoch 59/100\n",
      "----------\n",
      "train Loss: 0.0372 Acc: 0.9875\n",
      "val Loss: 0.2228 Acc: 0.9394\n",
      "\n",
      "Epoch 60/100\n",
      "----------\n",
      "train Loss: 0.0388 Acc: 0.9859\n",
      "val Loss: 0.2206 Acc: 0.9378\n",
      "\n",
      "Epoch 61/100\n",
      "----------\n",
      "train Loss: 0.0370 Acc: 0.9871\n",
      "val Loss: 0.2241 Acc: 0.9367\n",
      "\n",
      "Epoch 62/100\n",
      "----------\n",
      "train Loss: 0.0369 Acc: 0.9873\n",
      "val Loss: 0.2227 Acc: 0.9389\n",
      "\n",
      "Epoch 63/100\n",
      "----------\n",
      "train Loss: 0.0347 Acc: 0.9879\n",
      "val Loss: 0.2228 Acc: 0.9389\n",
      "\n",
      "Epoch 64/100\n",
      "----------\n",
      "train Loss: 0.0359 Acc: 0.9878\n",
      "val Loss: 0.2212 Acc: 0.9383\n",
      "\n",
      "Epoch 65/100\n",
      "----------\n",
      "train Loss: 0.0356 Acc: 0.9878\n",
      "val Loss: 0.2223 Acc: 0.9372\n",
      "\n",
      "Epoch 66/100\n",
      "----------\n",
      "train Loss: 0.0349 Acc: 0.9876\n",
      "val Loss: 0.2229 Acc: 0.9378\n",
      "\n",
      "Epoch 67/100\n",
      "----------\n",
      "train Loss: 0.0371 Acc: 0.9871\n",
      "val Loss: 0.2213 Acc: 0.9389\n",
      "\n",
      "Epoch 68/100\n",
      "----------\n",
      "train Loss: 0.0400 Acc: 0.9868\n",
      "val Loss: 0.2214 Acc: 0.9389\n",
      "\n",
      "Epoch 69/100\n",
      "----------\n",
      "train Loss: 0.0354 Acc: 0.9875\n",
      "val Loss: 0.2192 Acc: 0.9406\n",
      "\n",
      "Epoch 70/100\n",
      "----------\n",
      "train Loss: 0.0356 Acc: 0.9871\n",
      "val Loss: 0.2237 Acc: 0.9389\n",
      "\n",
      "Epoch 71/100\n",
      "----------\n",
      "train Loss: 0.0339 Acc: 0.9879\n",
      "val Loss: 0.2256 Acc: 0.9378\n",
      "\n",
      "Epoch 72/100\n",
      "----------\n",
      "train Loss: 0.0357 Acc: 0.9880\n",
      "val Loss: 0.2225 Acc: 0.9378\n",
      "\n",
      "Epoch 73/100\n",
      "----------\n",
      "train Loss: 0.0368 Acc: 0.9871\n",
      "val Loss: 0.2240 Acc: 0.9378\n",
      "\n",
      "Epoch 74/100\n",
      "----------\n",
      "train Loss: 0.0384 Acc: 0.9871\n",
      "val Loss: 0.2215 Acc: 0.9372\n",
      "\n",
      "Epoch 75/100\n",
      "----------\n",
      "train Loss: 0.0369 Acc: 0.9875\n",
      "val Loss: 0.2237 Acc: 0.9367\n",
      "\n",
      "Epoch 76/100\n",
      "----------\n",
      "train Loss: 0.0372 Acc: 0.9866\n",
      "val Loss: 0.2226 Acc: 0.9389\n",
      "\n",
      "Epoch 77/100\n",
      "----------\n",
      "train Loss: 0.0377 Acc: 0.9868\n",
      "val Loss: 0.2215 Acc: 0.9383\n",
      "\n",
      "Epoch 78/100\n",
      "----------\n",
      "train Loss: 0.0333 Acc: 0.9882\n",
      "val Loss: 0.2252 Acc: 0.9383\n",
      "\n",
      "Epoch 79/100\n",
      "----------\n",
      "train Loss: 0.0375 Acc: 0.9870\n",
      "val Loss: 0.2245 Acc: 0.9372\n",
      "\n",
      "Epoch 80/100\n",
      "----------\n",
      "train Loss: 0.0338 Acc: 0.9875\n",
      "val Loss: 0.2224 Acc: 0.9383\n",
      "\n",
      "Epoch 81/100\n",
      "----------\n",
      "train Loss: 0.0369 Acc: 0.9866\n",
      "val Loss: 0.2222 Acc: 0.9389\n",
      "\n",
      "Epoch 82/100\n",
      "----------\n",
      "train Loss: 0.0356 Acc: 0.9874\n",
      "val Loss: 0.2223 Acc: 0.9367\n",
      "\n",
      "Epoch 83/100\n",
      "----------\n",
      "train Loss: 0.0374 Acc: 0.9865\n",
      "val Loss: 0.2222 Acc: 0.9389\n",
      "\n",
      "Epoch 84/100\n",
      "----------\n",
      "train Loss: 0.0383 Acc: 0.9867\n",
      "val Loss: 0.2214 Acc: 0.9389\n",
      "\n",
      "Epoch 85/100\n",
      "----------\n",
      "train Loss: 0.0403 Acc: 0.9853\n",
      "val Loss: 0.2216 Acc: 0.9389\n",
      "\n",
      "Epoch 86/100\n",
      "----------\n",
      "train Loss: 0.0406 Acc: 0.9856\n",
      "val Loss: 0.2212 Acc: 0.9383\n",
      "\n",
      "Epoch 87/100\n",
      "----------\n",
      "train Loss: 0.0371 Acc: 0.9872\n",
      "val Loss: 0.2249 Acc: 0.9383\n",
      "\n",
      "Epoch 88/100\n",
      "----------\n",
      "train Loss: 0.0386 Acc: 0.9861\n",
      "val Loss: 0.2223 Acc: 0.9361\n",
      "\n",
      "Epoch 89/100\n",
      "----------\n",
      "train Loss: 0.0388 Acc: 0.9865\n",
      "val Loss: 0.2218 Acc: 0.9389\n",
      "\n",
      "Epoch 90/100\n",
      "----------\n",
      "train Loss: 0.0373 Acc: 0.9869\n",
      "val Loss: 0.2236 Acc: 0.9378\n",
      "\n",
      "Epoch 91/100\n",
      "----------\n",
      "train Loss: 0.0380 Acc: 0.9877\n",
      "val Loss: 0.2205 Acc: 0.9394\n",
      "\n",
      "Epoch 92/100\n",
      "----------\n",
      "train Loss: 0.0378 Acc: 0.9864\n",
      "val Loss: 0.2248 Acc: 0.9372\n",
      "\n",
      "Epoch 93/100\n",
      "----------\n",
      "train Loss: 0.0374 Acc: 0.9875\n",
      "val Loss: 0.2204 Acc: 0.9394\n",
      "\n",
      "Epoch 94/100\n",
      "----------\n",
      "train Loss: 0.0363 Acc: 0.9866\n",
      "val Loss: 0.2226 Acc: 0.9389\n",
      "\n",
      "Epoch 95/100\n",
      "----------\n",
      "train Loss: 0.0365 Acc: 0.9873\n",
      "val Loss: 0.2231 Acc: 0.9394\n",
      "\n",
      "Epoch 96/100\n",
      "----------\n",
      "train Loss: 0.0378 Acc: 0.9866\n",
      "val Loss: 0.2236 Acc: 0.9378\n",
      "\n",
      "Epoch 97/100\n",
      "----------\n",
      "train Loss: 0.0340 Acc: 0.9882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.2201 Acc: 0.9383\n",
      "\n",
      "Epoch 98/100\n",
      "----------\n",
      "train Loss: 0.0372 Acc: 0.9867\n",
      "val Loss: 0.2189 Acc: 0.9400\n",
      "\n",
      "Epoch 99/100\n",
      "----------\n",
      "train Loss: 0.0360 Acc: 0.9877\n",
      "val Loss: 0.2217 Acc: 0.9372\n",
      "\n",
      "Epoch 100/100\n",
      "----------\n",
      "train Loss: 0.0397 Acc: 0.9861\n",
      "val Loss: 0.2248 Acc: 0.9372\n",
      "\n",
      "Training complete in 45m 44s\n",
      "Best val Acc: 0.941111\n"
     ]
    }
   ],
   "source": [
    "model_ft, conf_matrix = train_model(\n",
    "    model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "    dataloaders, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# now prepare training sets of the remaining (bottom) classes\n",
    "trainset_tr = datasets['bottom']['train']\n",
    "valset_tr = datasets['bottom']['val']\n",
    "\n",
    "# can't stratify along classes since some have only one sample\n",
    "train_tr_sampler, train_tr_indices, val_tr_sampler, val_tr_indices = \\\n",
    "    get_train_and_val_sampler(trainset_tr, balanced_training=True, stratify=False)\n",
    "\n",
    "train_tr_loader = DataLoader(\n",
    "    trainset_tr, batch_size=batch_size, num_workers=4, sampler=train_tr_sampler)\n",
    "val_tr_loader = DataLoader(\n",
    "    valset_tr, batch_size=batch_size, num_workers=4, sampler=val_tr_sampler)\n",
    "\n",
    "dataloaders = {\"train\": train_tr_loader, \"val\": val_tr_loader}\n",
    "# dataset_sizes = {\"train\": len(train_tr_indices), \"val\": len(val_tr_indices)}\n",
    "\n",
    "# instaniate resnet again\n",
    "model_tr = models.resnet18(pretrained=True)\n",
    "\n",
    "# modify output layer\n",
    "model_tr.fc = nn.Linear(model_ft.fc.in_features, model_ft.fc.out_features)\n",
    "\n",
    "# copy fine tuned network weights\n",
    "model_tr.load_state_dict(copy.deepcopy(model_ft.state_dict()))\n",
    "\n",
    "# freeze all lower layers of the network\n",
    "# for param in model_tr.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# adapt final layer fully-connected layer (unfrozen)\n",
    "model_tr.fc = nn.Linear(num_ftrs, trainset_tr.n_classes)\n",
    "\n",
    "model_tr.to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_tr = optim.Adam(model_tr.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler_tr = optim.lr_scheduler.StepLR(optimizer_tr, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "train Loss: 3.4881 Acc: 0.2882\n",
      "val Loss: 2.7145 Acc: 0.3109\n",
      "\n",
      "Epoch 2/100\n",
      "----------\n",
      "train Loss: 1.7372 Acc: 0.6217\n",
      "val Loss: 1.7166 Acc: 0.5285\n",
      "\n",
      "Epoch 3/100\n",
      "----------\n",
      "train Loss: 1.0816 Acc: 0.7309\n",
      "val Loss: 1.2812 Acc: 0.6477\n",
      "\n",
      "Epoch 4/100\n",
      "----------\n",
      "train Loss: 0.7437 Acc: 0.8099\n",
      "val Loss: 1.1193 Acc: 0.6770\n",
      "\n",
      "Epoch 5/100\n",
      "----------\n",
      "train Loss: 0.5590 Acc: 0.8521\n",
      "val Loss: 0.9393 Acc: 0.7150\n",
      "\n",
      "Epoch 6/100\n",
      "----------\n",
      "train Loss: 0.4514 Acc: 0.8747\n",
      "val Loss: 0.9307 Acc: 0.7150\n",
      "\n",
      "Epoch 7/100\n",
      "----------\n",
      "train Loss: 0.3547 Acc: 0.9003\n",
      "val Loss: 0.8502 Acc: 0.7340\n",
      "\n",
      "Epoch 8/100\n",
      "----------\n",
      "train Loss: 0.3135 Acc: 0.9119\n",
      "val Loss: 0.7801 Acc: 0.7686\n",
      "\n",
      "Epoch 9/100\n",
      "----------\n",
      "train Loss: 0.2599 Acc: 0.9271\n",
      "val Loss: 0.7820 Acc: 0.7651\n",
      "\n",
      "Epoch 10/100\n",
      "----------\n",
      "train Loss: 0.2375 Acc: 0.9300\n",
      "val Loss: 0.7815 Acc: 0.7668\n",
      "\n",
      "Epoch 11/100\n",
      "----------\n",
      "train Loss: 0.2099 Acc: 0.9378\n",
      "val Loss: 0.7713 Acc: 0.7703\n",
      "\n",
      "Epoch 12/100\n",
      "----------\n",
      "train Loss: 0.2071 Acc: 0.9426\n",
      "val Loss: 0.7466 Acc: 0.7755\n",
      "\n",
      "Epoch 13/100\n",
      "----------\n",
      "train Loss: 0.1863 Acc: 0.9476\n",
      "val Loss: 0.7333 Acc: 0.7807\n",
      "\n",
      "Epoch 14/100\n",
      "----------\n",
      "train Loss: 0.1954 Acc: 0.9431\n",
      "val Loss: 0.7246 Acc: 0.7893\n",
      "\n",
      "Epoch 15/100\n",
      "----------\n",
      "train Loss: 0.1637 Acc: 0.9532\n",
      "val Loss: 0.7258 Acc: 0.7858\n",
      "\n",
      "Epoch 16/100\n",
      "----------\n",
      "train Loss: 0.1748 Acc: 0.9528\n",
      "val Loss: 0.7231 Acc: 0.7858\n",
      "\n",
      "Epoch 17/100\n",
      "----------\n",
      "train Loss: 0.1766 Acc: 0.9501\n",
      "val Loss: 0.7077 Acc: 0.7893\n",
      "\n",
      "Epoch 18/100\n",
      "----------\n",
      "train Loss: 0.1666 Acc: 0.9508\n",
      "val Loss: 0.7165 Acc: 0.7910\n",
      "\n",
      "Epoch 19/100\n",
      "----------\n",
      "train Loss: 0.1608 Acc: 0.9571\n",
      "val Loss: 0.7081 Acc: 0.7945\n",
      "\n",
      "Epoch 20/100\n",
      "----------\n",
      "train Loss: 0.1602 Acc: 0.9540\n",
      "val Loss: 0.7092 Acc: 0.7962\n",
      "\n",
      "Epoch 21/100\n",
      "----------\n",
      "train Loss: 0.1538 Acc: 0.9573\n",
      "val Loss: 0.7205 Acc: 0.7927\n",
      "\n",
      "Epoch 22/100\n",
      "----------\n",
      "train Loss: 0.1570 Acc: 0.9558\n",
      "val Loss: 0.7172 Acc: 0.7962\n",
      "\n",
      "Epoch 23/100\n",
      "----------\n",
      "train Loss: 0.1559 Acc: 0.9527\n",
      "val Loss: 0.7138 Acc: 0.7927\n",
      "\n",
      "Epoch 24/100\n",
      "----------\n",
      "train Loss: 0.1603 Acc: 0.9563\n",
      "val Loss: 0.7200 Acc: 0.7979\n",
      "\n",
      "Epoch 25/100\n",
      "----------\n",
      "train Loss: 0.1557 Acc: 0.9546\n",
      "val Loss: 0.7160 Acc: 0.7945\n",
      "\n",
      "Epoch 26/100\n",
      "----------\n",
      "train Loss: 0.1598 Acc: 0.9492\n",
      "val Loss: 0.7066 Acc: 0.7962\n",
      "\n",
      "Epoch 27/100\n",
      "----------\n",
      "train Loss: 0.1517 Acc: 0.9570\n",
      "val Loss: 0.7100 Acc: 0.7893\n",
      "\n",
      "Epoch 28/100\n",
      "----------\n",
      "train Loss: 0.1465 Acc: 0.9589\n",
      "val Loss: 0.7139 Acc: 0.7893\n",
      "\n",
      "Epoch 29/100\n",
      "----------\n",
      "train Loss: 0.1534 Acc: 0.9551\n",
      "val Loss: 0.7191 Acc: 0.7945\n",
      "\n",
      "Epoch 30/100\n",
      "----------\n",
      "train Loss: 0.1528 Acc: 0.9544\n",
      "val Loss: 0.7090 Acc: 0.7945\n",
      "\n",
      "Epoch 31/100\n",
      "----------\n",
      "train Loss: 0.1368 Acc: 0.9639\n",
      "val Loss: 0.7124 Acc: 0.7997\n",
      "\n",
      "Epoch 32/100\n",
      "----------\n",
      "train Loss: 0.1462 Acc: 0.9573\n",
      "val Loss: 0.7082 Acc: 0.7962\n",
      "\n",
      "Epoch 33/100\n",
      "----------\n",
      "train Loss: 0.1593 Acc: 0.9546\n",
      "val Loss: 0.7061 Acc: 0.7927\n",
      "\n",
      "Epoch 34/100\n",
      "----------\n",
      "train Loss: 0.1565 Acc: 0.9558\n",
      "val Loss: 0.7066 Acc: 0.7945\n",
      "\n",
      "Epoch 35/100\n",
      "----------\n",
      "train Loss: 0.1538 Acc: 0.9565\n",
      "val Loss: 0.7083 Acc: 0.7997\n",
      "\n",
      "Epoch 36/100\n",
      "----------\n",
      "train Loss: 0.1580 Acc: 0.9533\n",
      "val Loss: 0.6980 Acc: 0.7979\n",
      "\n",
      "Epoch 37/100\n",
      "----------\n",
      "train Loss: 0.1557 Acc: 0.9535\n",
      "val Loss: 0.7044 Acc: 0.7876\n",
      "\n",
      "Epoch 38/100\n",
      "----------\n",
      "train Loss: 0.1430 Acc: 0.9611\n",
      "val Loss: 0.7080 Acc: 0.7945\n",
      "\n",
      "Epoch 39/100\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9584\n",
      "val Loss: 0.7124 Acc: 0.7910\n",
      "\n",
      "Epoch 40/100\n",
      "----------\n",
      "train Loss: 0.1498 Acc: 0.9587\n",
      "val Loss: 0.7087 Acc: 0.7962\n",
      "\n",
      "Epoch 41/100\n",
      "----------\n",
      "train Loss: 0.1597 Acc: 0.9546\n",
      "val Loss: 0.7109 Acc: 0.7945\n",
      "\n",
      "Epoch 42/100\n",
      "----------\n",
      "train Loss: 0.1393 Acc: 0.9644\n",
      "val Loss: 0.7050 Acc: 0.8014\n",
      "\n",
      "Epoch 43/100\n",
      "----------\n",
      "train Loss: 0.1562 Acc: 0.9533\n",
      "val Loss: 0.7121 Acc: 0.7962\n",
      "\n",
      "Epoch 44/100\n",
      "----------\n",
      "train Loss: 0.1594 Acc: 0.9527\n",
      "val Loss: 0.7064 Acc: 0.7979\n",
      "\n",
      "Epoch 45/100\n",
      "----------\n",
      "train Loss: 0.1623 Acc: 0.9528\n",
      "val Loss: 0.7122 Acc: 0.7979\n",
      "\n",
      "Epoch 46/100\n",
      "----------\n",
      "train Loss: 0.1523 Acc: 0.9566\n",
      "val Loss: 0.7006 Acc: 0.7945\n",
      "\n",
      "Epoch 47/100\n",
      "----------\n",
      "train Loss: 0.1465 Acc: 0.9594\n",
      "val Loss: 0.7187 Acc: 0.7945\n",
      "\n",
      "Epoch 48/100\n",
      "----------\n",
      "train Loss: 0.1552 Acc: 0.9542\n",
      "val Loss: 0.7020 Acc: 0.7962\n",
      "\n",
      "Epoch 49/100\n",
      "----------\n",
      "train Loss: 0.1545 Acc: 0.9575\n",
      "val Loss: 0.7072 Acc: 0.7893\n",
      "\n",
      "Epoch 50/100\n",
      "----------\n",
      "train Loss: 0.1572 Acc: 0.9516\n",
      "val Loss: 0.7099 Acc: 0.7979\n",
      "\n",
      "Epoch 51/100\n",
      "----------\n",
      "train Loss: 0.1552 Acc: 0.9561\n",
      "val Loss: 0.7101 Acc: 0.7945\n",
      "\n",
      "Epoch 52/100\n",
      "----------\n",
      "train Loss: 0.1615 Acc: 0.9506\n",
      "val Loss: 0.7092 Acc: 0.8014\n",
      "\n",
      "Epoch 53/100\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9597\n",
      "val Loss: 0.7039 Acc: 0.7945\n",
      "\n",
      "Epoch 54/100\n",
      "----------\n",
      "train Loss: 0.1507 Acc: 0.9565\n",
      "val Loss: 0.7116 Acc: 0.8031\n",
      "\n",
      "Epoch 55/100\n",
      "----------\n",
      "train Loss: 0.1454 Acc: 0.9584\n",
      "val Loss: 0.7157 Acc: 0.7927\n",
      "\n",
      "Epoch 56/100\n",
      "----------\n",
      "train Loss: 0.1533 Acc: 0.9570\n",
      "val Loss: 0.7111 Acc: 0.7927\n",
      "\n",
      "Epoch 57/100\n",
      "----------\n",
      "train Loss: 0.1558 Acc: 0.9575\n",
      "val Loss: 0.7112 Acc: 0.7945\n",
      "\n",
      "Epoch 58/100\n",
      "----------\n",
      "train Loss: 0.1548 Acc: 0.9539\n",
      "val Loss: 0.7082 Acc: 0.7945\n",
      "\n",
      "Epoch 59/100\n",
      "----------\n",
      "train Loss: 0.1463 Acc: 0.9616\n",
      "val Loss: 0.7034 Acc: 0.7997\n",
      "\n",
      "Epoch 60/100\n",
      "----------\n",
      "train Loss: 0.1593 Acc: 0.9556\n",
      "val Loss: 0.7061 Acc: 0.7962\n",
      "\n",
      "Epoch 61/100\n",
      "----------\n",
      "train Loss: 0.1434 Acc: 0.9577\n",
      "val Loss: 0.7028 Acc: 0.7945\n",
      "\n",
      "Epoch 62/100\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9620\n",
      "val Loss: 0.7071 Acc: 0.7927\n",
      "\n",
      "Epoch 63/100\n",
      "----------\n",
      "train Loss: 0.1530 Acc: 0.9590\n",
      "val Loss: 0.7052 Acc: 0.8014\n",
      "\n",
      "Epoch 64/100\n",
      "----------\n",
      "train Loss: 0.1589 Acc: 0.9539\n",
      "val Loss: 0.7044 Acc: 0.7979\n",
      "\n",
      "Epoch 65/100\n",
      "----------\n",
      "train Loss: 0.1556 Acc: 0.9575\n",
      "val Loss: 0.7085 Acc: 0.7962\n",
      "\n",
      "Epoch 66/100\n",
      "----------\n",
      "train Loss: 0.1612 Acc: 0.9565\n",
      "val Loss: 0.7039 Acc: 0.7979\n",
      "\n",
      "Epoch 67/100\n",
      "----------\n",
      "train Loss: 0.1473 Acc: 0.9575\n",
      "val Loss: 0.7137 Acc: 0.7979\n",
      "\n",
      "Epoch 68/100\n",
      "----------\n",
      "train Loss: 0.1452 Acc: 0.9615\n",
      "val Loss: 0.7074 Acc: 0.7962\n",
      "\n",
      "Epoch 69/100\n",
      "----------\n",
      "train Loss: 0.1450 Acc: 0.9587\n",
      "val Loss: 0.7037 Acc: 0.7962\n",
      "\n",
      "Epoch 70/100\n",
      "----------\n",
      "train Loss: 0.1586 Acc: 0.9521\n",
      "val Loss: 0.7062 Acc: 0.7979\n",
      "\n",
      "Epoch 71/100\n",
      "----------\n",
      "train Loss: 0.1556 Acc: 0.9559\n",
      "val Loss: 0.7109 Acc: 0.7979\n",
      "\n",
      "Epoch 72/100\n",
      "----------\n",
      "train Loss: 0.1566 Acc: 0.9547\n",
      "val Loss: 0.7190 Acc: 0.7945\n",
      "\n",
      "Epoch 73/100\n",
      "----------\n",
      "train Loss: 0.1552 Acc: 0.9565\n",
      "val Loss: 0.7088 Acc: 0.7927\n",
      "\n",
      "Epoch 74/100\n",
      "----------\n",
      "train Loss: 0.1512 Acc: 0.9582\n",
      "val Loss: 0.7057 Acc: 0.7927\n",
      "\n",
      "Epoch 75/100\n",
      "----------\n",
      "train Loss: 0.1459 Acc: 0.9604\n",
      "val Loss: 0.7050 Acc: 0.7945\n",
      "\n",
      "Epoch 76/100\n",
      "----------\n",
      "train Loss: 0.1585 Acc: 0.9575\n",
      "val Loss: 0.7080 Acc: 0.7945\n",
      "\n",
      "Epoch 77/100\n",
      "----------\n",
      "train Loss: 0.1566 Acc: 0.9575\n",
      "val Loss: 0.7114 Acc: 0.7945\n",
      "\n",
      "Epoch 78/100\n",
      "----------\n",
      "train Loss: 0.1558 Acc: 0.9570\n",
      "val Loss: 0.7091 Acc: 0.7910\n",
      "\n",
      "Epoch 79/100\n",
      "----------\n",
      "train Loss: 0.1551 Acc: 0.9556\n",
      "val Loss: 0.7109 Acc: 0.7962\n",
      "\n",
      "Epoch 80/100\n",
      "----------\n",
      "train Loss: 0.1570 Acc: 0.9551\n",
      "val Loss: 0.7076 Acc: 0.7962\n",
      "\n",
      "Epoch 81/100\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.9594\n",
      "val Loss: 0.7071 Acc: 0.7979\n",
      "\n",
      "Epoch 82/100\n",
      "----------\n",
      "train Loss: 0.1488 Acc: 0.9570\n",
      "val Loss: 0.7079 Acc: 0.8014\n",
      "\n",
      "Epoch 83/100\n",
      "----------\n",
      "train Loss: 0.1562 Acc: 0.9540\n",
      "val Loss: 0.7040 Acc: 0.7962\n",
      "\n",
      "Epoch 84/100\n",
      "----------\n",
      "train Loss: 0.1553 Acc: 0.9558\n",
      "val Loss: 0.7048 Acc: 0.7979\n",
      "\n",
      "Epoch 85/100\n",
      "----------\n",
      "train Loss: 0.1541 Acc: 0.9568\n",
      "val Loss: 0.7137 Acc: 0.7945\n",
      "\n",
      "Epoch 86/100\n",
      "----------\n",
      "train Loss: 0.1573 Acc: 0.9554\n",
      "val Loss: 0.7079 Acc: 0.7962\n",
      "\n",
      "Epoch 87/100\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.9552\n",
      "val Loss: 0.7007 Acc: 0.7997\n",
      "\n",
      "Epoch 88/100\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9563\n",
      "val Loss: 0.7178 Acc: 0.7962\n",
      "\n",
      "Epoch 89/100\n",
      "----------\n",
      "train Loss: 0.1532 Acc: 0.9559\n",
      "val Loss: 0.7138 Acc: 0.8014\n",
      "\n",
      "Epoch 90/100\n",
      "----------\n",
      "train Loss: 0.1493 Acc: 0.9570\n",
      "val Loss: 0.7087 Acc: 0.8014\n",
      "\n",
      "Epoch 91/100\n",
      "----------\n",
      "train Loss: 0.1617 Acc: 0.9530\n",
      "val Loss: 0.7134 Acc: 0.7945\n",
      "\n",
      "Epoch 92/100\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.9606\n",
      "val Loss: 0.7139 Acc: 0.7945\n",
      "\n",
      "Epoch 93/100\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9584\n",
      "val Loss: 0.7101 Acc: 0.7962\n",
      "\n",
      "Epoch 94/100\n",
      "----------\n",
      "train Loss: 0.1526 Acc: 0.9571\n",
      "val Loss: 0.7036 Acc: 0.8014\n",
      "\n",
      "Epoch 95/100\n",
      "----------\n",
      "train Loss: 0.1460 Acc: 0.9620\n",
      "val Loss: 0.7077 Acc: 0.7945\n",
      "\n",
      "Epoch 96/100\n",
      "----------\n",
      "train Loss: 0.1520 Acc: 0.9558\n",
      "val Loss: 0.7083 Acc: 0.8031\n",
      "\n",
      "Epoch 97/100\n",
      "----------\n",
      "train Loss: 0.1599 Acc: 0.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.7054 Acc: 0.7997\n",
      "\n",
      "Epoch 98/100\n",
      "----------\n",
      "train Loss: 0.1475 Acc: 0.9566\n",
      "val Loss: 0.7118 Acc: 0.7979\n",
      "\n",
      "Epoch 99/100\n",
      "----------\n",
      "train Loss: 0.1534 Acc: 0.9575\n",
      "val Loss: 0.7124 Acc: 0.7979\n",
      "\n",
      "Epoch 100/100\n",
      "----------\n",
      "train Loss: 0.1623 Acc: 0.9547\n",
      "val Loss: 0.7072 Acc: 0.7927\n",
      "\n",
      "Training complete in 15m 22s\n",
      "Best val Acc: 0.803109\n"
     ]
    }
   ],
   "source": [
    "model_tr, conf_matrix_tr= train_model(\n",
    "    model_tr, criterion, optimizer_tr, exp_lr_scheduler_tr,\n",
    "    dataloaders, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4359953320>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD7CAYAAACITjpPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAASpklEQVR4nO3df4xdZZ3H8fenM20HZKVTJGVsCR1Dq1vMYtkRO7DZEKuxILGYuARCtCtNGhN28WcU5A+yf6zRrFEhcWEbQashIFZ22xAXFivEbITCQFmkrZRZxkrrlNYwoKsWOtPv/nHPhds+8/Oee++5Pz6vpJl7ftx7vn3m9tvvec5znqOIwMys0ryiAzCz5uPEYGYJJwYzSzgxmFnCicHMEk4MZpaoW2KQtE7Sc5KGJd1Qr+OYWe2pHuMYJHUB+4APAgeAJ4CrI2JPzQ9mZjXXXafPvRAYjogXACTdA6wHJk0MC+b1xCnz/oKYmKhTOGb2B8Z+FxFnzmbfeiWGpcCLFcsHgPdV7iBpE7AJoGfeaQye/lEmxsbqFI6Z/TS27p/tvoV1PkbE5ogYiIiB+cfnOymYNZF6JYaDwNkVy8uydWbWAuqVGJ4AVkjql7QAuArYPts3/3n9hbkDmNfTw7yentyfY9aJ6tLHEBHjkv4BeBDoAu6MiN2zff8p2x7PHcPxo0dzf4ZZp6pX5yMR8RPgJ/X6fDOrH498NLOEE4OZJZwYzCzhxGBmCScGM0s4MZhZwonBzBJODGaWcGIws0RbJIau3l66enuLDsOsbbRFYjCz2mqLxDAxNsbE2JirBrMaaYvEUDYxNsZL11/ES9dfVHQoZi2trRKDmdVG3W67LsqSW38BQHffWQCMjx46Yfu8nh7P1WA2A1cMZpZou4qh7ORKoczVgtnMXDGYWaJjEsOf119Yk0lmLdXOl4m7+88pOoRCdExiMLPZ65jEcMq2xzll2+Me41AH7fywoPGRWT+8qa20befjycrPmFhy6y/eKH3b+QttlkfHVAxmNnsdUzFUXqZ0pWA2PVcMZpZwYgD23THAvjsGZr1/3vkf/FzNztYKv38nBjNLVN3HIOls4PvAEiCAzRFxi6TFwA+B5cCvgSsjoqlP6lduHJrT/nn7KDwsu7O1wu8/T8UwDnw+IlYBa4DrJK0CbgB2RMQKYEe2bGYtpOrEEBGjEfFU9voPwF5gKbAe2JLttgW4Im+QZtZYNblcKWk5sBrYCSyJiNFs0yFKpxqTvWcTsAmgh1NrEUZNvb7uvQAseOCJgiMxa7zcnY+STgN+DHwmIn5fuS0iglL/QyIiNkfEQEQMzGdh3jDMrIZyVQyS5lNKCndFxH3Z6pck9UXEqKQ+4HDeIIvgSsE6WdUVgyQBdwB7I+IbFZu2Axuy1xuAbdWHZ2ZFyFMxXAx8HPilpKezdV8GvgrcK2kjsB+4Ml+IZtZoVSeGiPhvQFNsXlvt55pZ8TzycY5ueuHpmXfKoVmGyzZLHPBmLEU/irCZ2qSsa9VKulatrPnndszdlbXyz+94DxOXXABA1yNP1fzzm2VUXLPEARWxFBxTM7VJ2cSefXX5XFcMZpZwxVCFcqXw6jVrADj9rseKDMes5lwxmFnCFUMO5UrhL58sNePevx4vMhyzmnHFYGYJVww14ErB2o0rBjNLODGYWcKJwcwSTgxmlnBiMLOEE4OZJZwYzCzhxGBmCScGM0s4MRRg+AerGf7B6qLDsCo144QttebEYGYJ3ytRgHM/vqvoECyHZpzJqdZcMZhZwonBzBJODGaWcGIws4QTQ5PR6vPQ6vOKDqMjdMJlx2o5MZhZIvflSkldwBBwMCIul9QP3AOcATwJfDwiXs97nE4Ru3YD0N13FgDjo4eKDKetdcJlx2rVomL4NLC3YvlrwDcj4lxgDNhYg2OYWQPlSgySlgEfBr6TLQt4P7A122ULcEWeY3Sq8dFDrhasMHkrhm8BXwSOZ8tnAK9ERHna5APA0sneKGmTpCFJQ8d4LWcYZlZLVScGSZcDhyPiyWreHxGbI2IgIgbms7DaMMysDvJ0Pl4MfETSZUAP8FbgFmCRpO6salgGHMwfppk1UtUVQ0TcGBHLImI5cBXws4i4BngY+Fi22wZgW+4ozayh6jGO4UvA5yQNU+pzuKMOxzCzOqrJbdcR8QjwSPb6BeDCWnyumRXDIx/NLOHEYGYJJ4Y2MPKVQUa+Mlh0GDaFVrxZy4nBzBKe87EN9H/50aJDsGm04s1arhjaTAyeTwyeX3QYDdXsZbpPJcysLfhUos3o0f8Bqp/Poau3F4CJsbHaBlZHzV6qN3t8k3HFYGYJVwxtqlwpzLUCaKVKwerHFYOZJVwxtDlXAFYNVwxmlnBiMLOEE4OZJZwYzCzhxNDB/Dg8m4oTg5klnBjmqDxgqB3Ert3Ert0c+ZTncqhW0TdIdfX21uU76XEMczQxNvbGF6EVx8BP5szbH6W7/xwAxkf2FxxNayn6O1CvcSquGMws0ZEVQ97/8Yv+X6IeypXC//3d+wA47Uc7iwzHCuaKwcwSHVkxtOP/+LXiSsHAFYOZTcKJwcwSuRKDpEWStkr6laS9kgYlLZb0kKTns5/tc+HfrEPkrRhuAR6IiHcB5wN7gRuAHRGxAtiRLZtZC6k6MUg6HfhbsqdZR8TrEfEKsB7Yku22Bbgib5Bm1lh5KoZ+4AjwXUm7JH1H0luAJRExmu1zCFgy2ZslbZI0JGnoGK/lCMPMai1PYugGLgBui4jVwB856bQhIgKIyd4cEZsjYiAiBuazMEcYZlZreRLDAeBARJQvfG+llChektQHkP08nC9EM2u0qhNDRBwCXpT0zmzVWmAPsB3YkK3bAGzLFaGZNVzekY//CNwlaQHwAvBJSsnmXkkbgf3AlTmPYWYNlisxRMTTwMAkm9bm+VxrLd19Z835UXjW3Dzy0XIbHz1U+IQlVltODGaW6Mi7K632fMdqe3HFYGYJJwYzSzgxmFnCicHMEk4MZpZwYjCzhBODmSWcGMws4cRA/Z7/Z+2pE4Z/OzGYWcJDoqnfg0Ftet19ZwG03J2ZnTD82xWDmSVcMVhhmqVSmNfT0xFVwFy4YrCO56SQcmIws4QTg5klnBjMLOHEYE3p5WsHefnawaLD6FhODGaW8OXKNlMeqtvqPe2L73y06BA6miuGNnP86NFJk8JkY/tbZcx/16qVdK1aWXQYHcWJwcwSPpXoEJNVEa1yujGxZx8A3f3nADA+sr/IcDqCKwYzS+RKDJI+K2m3pGcl3S2pR1K/pJ2ShiX9MHvgrVlu4yP7GR/ZX3XfSKv0qTSDqhODpKXA9cBARLwb6AKuAr4GfDMizgXGgI21CNTMGifvqUQ3cIqkbuBUYBR4P7A1274FuCLnMcxOUL7y8vq69/L6uvfO+X02s6oTQ0QcBL4O/IZSQngVeBJ4JSLGs90OAEsne7+kTZKGJA0d47VqwzCzOqj6qoSkXmA90A+8AvwIWDfb90fEZmAzwFu1OKqNwzrXggeeADyfQj3kOZX4ADASEUci4hhwH3AxsCg7tQBYBhzMGaPZtJwUai9PYvgNsEbSqZIErAX2AA8DH8v22QBsyxeimTVanj6GnZQ6GZ8Cfpl91mbgS8DnJA0DZwB31CBOs1nR6vPQ6vOKDqPl5Rr5GBE3AzeftPoF4MI8n2tmxfKQaGsrsWt30SG0BQ+JNrOEE4O1tfJDbWxunBgMaN/7CJrl2RWtxonBzBLufDSgMwYJlZ9o7meVzswVg5klXDFYxyhXCq4cZuaKwcwSrhis45QrhQd/+zQAH3r7e4oMpym5YjCzhCuGNtMuD5xphHKl0N1/jmeePokTQ5txQpi78gSzMH37dVLS9amEmSVcMZjxZhUwXVXQCZVCmSsGM0u4YjCr0ElVwXRcMZhZomMSQ7veVtws2rV9X75/5Ruv2/XvOJmOOZVwiVhf7dq+iy/f98brdv07TqZjKgYzmz0nBrNZ6qSp6Z0YzCzRMX0MZnmVp6bvWlXqkJzYs2+63VuaKwYzS7hiMJul8qXKcqXQzk/ZnrFikHSnpMOSnq1Yt1jSQ5Kez372Zusl6VZJw5KekXRBPYMvWldv7xvThFn7O3706AmJ4PjRo0xccgETl7Tf13w2pxLfA9adtO4GYEdErAB2ZMsAlwIrsj+bgNtqE6aZNdKMpxIR8XNJy09avR64JHu9BXiE0lOu1wPfj4gAHpO0SFJfRIzWKuBm4slEreuRp4oOoS6q7XxcUvGP/RCwJHu9FHixYr8D2bqEpE2ShiQNHeO1KsMws3rIfVUiqw6iivdtjoiBiBiYz8K8YZhZDVWbGF6S1AeQ/TycrT8InF2x37JsnZm1kGoTw3ZgQ/Z6A7CtYv0nsqsTa4BX27V/wWw6XatWvjEQqhXN2Pko6W5KHY1vk3QAuBn4KnCvpI3AfuDKbPefAJcBw8CfgE/WIWYzq7PZXJW4eopNayfZN4Dr8gZl1upafRCUh0Sb1VkrTvDixGBmCd8r0YI66cEnra5Vf0euGMws4YqhBbXq/0LWOlwxmFnCicGsICNfGSw6hCk5MZgVpP/LjxYdwpScGMws4cRgJ2jFwThWe04MZpbw5Uo7gS+FGrhiMLNJODGYTaOo/pbpHofXiH4gJwYzS7iPwWwaRfW5lB+HN9l8Do2IyRVDG/AlxvZVVGJyYjCzhE8l2oAvMVqtuWIws4QTg+Xm/o3a6+47i+6+sybd9tsvXMRvv3BRXY/vxGBmCfcxWG7u46i98dFDU257+9d/AcCRTw1y5u31uXXbFcMctfLThabS3X9O0SFYFc68/VFevnaQl6+t/YQvTgxmlvCpxByVnzDUTsZH9hcdglVp8Z0FnUpIulPSYUnPVqz7F0m/kvSMpH+XtKhi242ShiU9J+lDdYnazOpqNqcS3wPWnbTuIeDdEfFXwD7gRgBJq4CrgPOy9/yrpK6aRWtmDTFjYoiInwMvn7TuvyJiPFt8DFiWvV4P3BMRr0XECKWnXl9Yw3jNrAFq0fl4LfCf2eulwIsV2w5k6xKSNkkakjR0jNdqEIZZdTxAK5UrMUi6CRgH7prreyNic0QMRMTAfBbmCcMsF4/DSFV9VULS3wOXA2sjIrLVB4GzK3Zblq0zsxZSVcUgaR3wReAjEfGnik3bgaskLZTUD6wAHs8fppk10owVg6S7gUuAt0k6ANxM6SrEQuAhSQCPRcSnImK3pHuBPZROMa6LiIl6BW9m9aE3zwIKDEI6AvwR+F3RsWTeRvPEAs0VTzPFAs0VT7PHck5EnDmbNzdFYgCQNBQRA0XHAc0VCzRXPM0UCzRXPO0Ui++VMLOEE4OZJZopMWwuOoAKzRQLNFc8zRQLNFc8bRNL0/QxmFnzaKaKwcyahBODmSWaIjFIWpfN3zAs6YYGH/tsSQ9L2iNpt6RPZ+sXS3pI0vPZz94GxtQlaZek+7Plfkk7s/b5oaQFDYxlkaSt2fwbeyUNFtU2kj6b/Y6elXS3pJ5Gts0Uc5NM2hYquTWL6xlJFzQglprNk1J4Ysjma/g2cCmwCrg6m9ehUcaBz0fEKmANcF12/BuAHRGxAtiRLTfKp4G9FctfA74ZEecCY8DGBsZyC/BARLwLOD+Lq+FtI2kpcD0wEBHvBroozf3RyLb5HuncJFO1xaWUbglYAWwCbmtALLWbJyUiCv0DDAIPVizfCNxYYDzbgA8CzwF92bo+4LkGHX8ZpS/Y+4H7AVEawdY9WXvVOZbTgRGyTuqK9Q1vG968pX8xpaH89wMfanTbAMuBZ2dqC+DfgKsn269esZy07aPAXdnrE/5NAQ8Cg9N9duEVA3OYw6HeJC0HVgM7gSURMZptOgQsaVAY36J0g9rxbPkM4JV4c2KcRrZPP3AE+G52avMdSW+hgLaJiIPA14HfAKPAq8CTFNc2ZVO1RdHf66rmSSlrhsTQFCSdBvwY+ExE/L5yW5TSbN2v60q6HDgcEU/W+1iz1A1cANwWEasp3c9ywmlDA9uml9IMYf3A24G3kJbShWpUW8wkzzwpZc2QGAqfw0HSfEpJ4a6IuC9b/ZKkvmx7H3C4AaFcDHxE0q+BeyidTtwCLJJUvhO2ke1zADgQETuz5a2UEkURbfMBYCQijkTEMeA+Su1VVNuUTdUWhXyvK+ZJuSZLVFXF0gyJ4QlgRda7vIBSJ8n2Rh1cpfvG7wD2RsQ3KjZtBzZkrzdQ6nuoq4i4MSKWRcRySu3ws4i4BngY+FgjY8niOQS8KOmd2aq1lG6pb3jbUDqFWCPp1Ox3Vo6lkLapMFVbbAc+kV2dWAO8WnHKURc1nSel3p1Gs+xEuYxSL+r/Ajc1+Nh/Q6n8ewZ4OvtzGaVz+x3A88BPgcUNjusS4P7s9TuyX+Qw8CNgYQPjeA8wlLXPfwC9RbUN8E/Ar4BngR9QmhOkYW0D3E2pf+MYpWpq41RtQanT+NvZd/qXlK6m1DuWYUp9CeXv8e0V+9+UxfIccOlMn+8h0WaWaIZTCTNrMk4MZpZwYjCzhBODmSWcGMws4cRgZgknBjNL/D9NMz/M0vFvKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(conf_matrix_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_transfer.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4359c106d8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
