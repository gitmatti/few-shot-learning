{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from torchvision.datasets import VisionDataset, CelebA\n",
    "from torchvision.datasets.utils import check_integrity, download_and_extract_archive, download_file_from_google_drive, verify_str_arg\n",
    "\n",
    "from functools import partial\n",
    "import torch\n",
    "import os\n",
    "import PIL\n",
    "import pandas\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import copy\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # transforms.RandomResizedCrop((80,60)),\n",
    "        transforms.Resize((80,60)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((80,60)),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionProductImages(VisionDataset):\n",
    "    \"\"\"TODO\n",
    "    \"\"\"\n",
    "    base_folder = 'fashion-product-images-small' # 'fashion-dataset'\n",
    "    # base_folder_small_dataset = 'fashion-product-images-small'\n",
    "    url = None\n",
    "    filename = \"fashion-product-images-dataset.zip\" # 'fashion-product-images-small.zip'\n",
    "    tgz_md5 = None\n",
    "\n",
    "    file_list = [\n",
    "        # File ID                         MD5 Hash                            Filename\n",
    "        (\"xxx\", \"xxx\", \"fashion-product-images-dataset.zip\"),\n",
    "        (\"xxx\", \"xxx\", \"fashion-product-images-small.zip\"),\n",
    "        (\"xxx\", \"xxx\", \"fashion-dataset\"),\n",
    "        (\"xxx\", \"xxx\", \"fashion-product-images-small\"),\n",
    "        (\"xxx\", \"xxx\", \"styles.csv\"),\n",
    "        (\"xxx\", \"xxx\", \"images.csv\"),\n",
    "    ]\n",
    "    \n",
    "    top20_classes = [\n",
    "        \"Jeans\", \"Perfume and Body Mist\", \"Formal Shoes\",\n",
    "        \"Socks\", \"Backpacks\", \"Belts\", \"Briefs\",\n",
    "        \"Sandals\", \"Flip Flops\", \"Wallets\", \"Sunglasses\",\n",
    "        \"Heels\", \"Handbags\", \"Tops\", \"Kurtas\",\n",
    "        \"Sports Shoes\", \"Watches\", \"Casual Shoes\", \"Shirts\",\n",
    "        \"Tshirts\"]  # actually 21?\n",
    "\n",
    "    def __init__(self, root, split=\"train\", target_type=\"articleType\", transform=None,\n",
    "                 target_transform=None, download=False, small_dataset=False):\n",
    "        super(FashionProductImages, self).__init__(\n",
    "            root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        self.split = split\n",
    "        self.target_type = target_type\n",
    "\n",
    "        # TODO.extension: should a list of target types be allowed?\n",
    "        self.target_type = target_type\n",
    "        \n",
    "        # TODO.extension: allow for usage of small dataset\n",
    "        #if small_dataset:\n",
    "        #    base_folder = self.base_folder_small_dataset\n",
    "        #else:\n",
    "        #    base_folder = self.base_folder\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        # TODO.not_implemented\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        \n",
    "        fn = partial(os.path.join, self.root, self.base_folder)\n",
    "\n",
    "        # TODO.refactor: refer to class attributes instead of \"styles.csv\"\n",
    "        # TODO.edge_case: see whether the styles.csv are identical/have the same corrupt rows for small and full dataset\n",
    "        with open(fn(\"styles.csv\")) as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            column_names = next(csv_reader)\n",
    "            \n",
    "        column_names.append(column_names[-1] + '2')\n",
    "        \n",
    "        # TODO.refactor: clean up column names, potentially merge last to columns\n",
    "        # TODO.refactor: column year is not parsed as integer - why?\n",
    "        self.df_meta = pandas.read_csv(fn(\"styles.csv\"), names=column_names, skiprows=1)\n",
    "        \n",
    "        # check if the images from 'styles.csv' are actually present in the 'images' folder\n",
    "        images = os.listdir(fn(\"images\"))\n",
    "        self.samples = self.df_meta.loc[\n",
    "            (self.df_meta[self.target_type].isin(self.top20_classes))\n",
    "            & (self.df_meta[\"id\"].apply(lambda x: str(x) +\".jpg\").isin(images))\n",
    "        ]\n",
    "                \n",
    "        self.targets = self.samples[self.target_type]\n",
    "        self.target_codec = LabelEncoder()\n",
    "        self.target_codec.fit(self.targets)\n",
    "        \n",
    "        # TODO.decision: are additional codecs necessary? \n",
    "        # self.article_codec = LabelEncoder()\n",
    "        # self.gender_codec = LabelEncoder()\n",
    "        # self.master_category_codec = LabelEncoder()\n",
    "        # self.season_codec = LabelEncoder()\n",
    "        # self.article_codec.fit(self.metadata.loc[:, \"articleType\"])\n",
    "        \n",
    "        # TODO.decision: prepare indices here or when __getitem__ is called?\n",
    "        self.target_indices = self.target_codec.transform(self.targets)\n",
    "        \n",
    "        self.n_classes = len(self.target_codec.classes_)\n",
    "        \n",
    "        # TODO.goal: test and training set split\n",
    "        # TODO.goal: fine-tune vs transfer classes\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # TODO.check: which images are not RGB?\n",
    "        # TODO.check: which images are not 80x60?\n",
    "        sample = str(self.samples[\"id\"].iloc[index]) + \".jpg\"\n",
    "        X = PIL.Image.open(os.path.join(self.root, self.base_folder, \"images\", sample)).convert(\"RGB\")\n",
    "        target = self.target_indices[index]\n",
    "        \n",
    "        # TODO.extension: allow returning one-hot representation of target\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return X, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def download(self):\n",
    "        # TODO.recycle: check this and compare to e.g. MNIST/CIFAR\n",
    "        # TODO.tailored: how to download from Kaggle\n",
    "        #if self._check_integrity():\n",
    "        #    print('Files already downloaded and verified')\n",
    "        #    return\n",
    "\n",
    "        #for (file_id, md5, filename) in self.file_list:\n",
    "        #    download_file_from_google_drive(file_id, os.path.join(self.root, self.base_folder), filename, md5)\n",
    "\n",
    "        #with zipfile.ZipFile(os.path.join(self.root, self.base_folder, \"img_align_celeba.zip\"), \"r\") as f:\n",
    "        #    f.extractall(os.path.join(self.root, self.base_folder))\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _check_integrity(self):\n",
    "\n",
    "        # TODO.recycle: check this and compare to e.g. MNIST/CIFAR\n",
    "\n",
    "        #for (_, md5, filename) in self.file_list:\n",
    "        #    fpath = os.path.join(self.root, self.base_folder, filename)\n",
    "        #    _, ext = os.path.splitext(filename)\n",
    "        #    # Allow original archive to be deleted (zip and 7z)\n",
    "        #    # Only need the extracted images\n",
    "        #    if ext not in [\".zip\", \".7z\"] and not check_integrity(fpath, md5):\n",
    "        #        return False\n",
    "\n",
    "        # Should check a hash of the images\n",
    "        #return os.path.isdir(os.path.join(self.root, self.base_folder, \"img_align_celeba\"))\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        # lines = [\"Target type: {target_type}\", \"Split: {split}\"]\n",
    "        # return '\\n'.join(lines).format(**self.__dict__)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = FashionProductImages(\"~/data\", transform=data_transforms[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fashion[10]\n",
    "# X, y = fashion[10:15] # fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33149"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(fashion) * 0.75)\n",
    "trainset, valset = random_split(fashion, [train_size, len(fashion) - train_size])\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(valset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "dataset_sizes = {\"train\": len(trainset), \"val\": len(valset)}\n",
    "\n",
    "# data_loader = DataLoader(fashion, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fashion)):\n",
    "    X, y = fashion[i]\n",
    "    # print(y.shape)\n",
    "    # if not isinstance(y, int):\n",
    "    #    import pdb; pdb.set_trace()\n",
    "    if not (X.shape[1]==80 and X.shape[2]==60):\n",
    "        import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "512\n",
      "576\n",
      "640\n",
      "704\n",
      "768\n",
      "832\n",
      "896\n",
      "960\n",
      "1024\n",
      "1088\n",
      "1152\n",
      "1216\n",
      "1280\n",
      "1344\n",
      "1408\n",
      "1472\n",
      "1536\n",
      "1600\n",
      "1664\n",
      "1728\n",
      "1792\n",
      "1856\n",
      "1920\n",
      "1984\n",
      "2048\n",
      "2112\n",
      "2176\n",
      "2240\n",
      "2304\n",
      "2368\n",
      "2432\n",
      "2496\n",
      "2560\n",
      "2624\n",
      "2688\n",
      "2752\n",
      "2816\n",
      "2880\n",
      "2944\n",
      "3008\n",
      "3072\n",
      "3136\n",
      "3200\n",
      "3264\n",
      "3328\n",
      "3392\n",
      "3456\n",
      "3520\n",
      "3584\n",
      "3648\n",
      "3712\n",
      "3776\n",
      "3840\n",
      "3904\n",
      "3968\n",
      "4032\n",
      "4096\n",
      "4160\n",
      "4224\n",
      "4288\n",
      "4352\n",
      "4416\n",
      "4480\n",
      "4544\n",
      "4608\n",
      "4672\n",
      "4736\n",
      "4800\n",
      "4864\n",
      "4928\n",
      "4992\n",
      "5056\n",
      "5120\n",
      "5184\n",
      "5248\n",
      "5312\n",
      "5376\n",
      "5440\n",
      "5504\n",
      "5568\n",
      "5632\n",
      "5696\n",
      "5760\n",
      "5824\n",
      "5888\n",
      "5952\n",
      "6016\n",
      "6080\n",
      "6144\n",
      "6208\n",
      "6272\n",
      "6336\n",
      "6400\n",
      "6464\n",
      "6528\n",
      "6592\n",
      "6656\n",
      "6720\n",
      "6784\n",
      "6848\n",
      "6912\n",
      "6976\n",
      "7040\n",
      "7104\n",
      "7168\n",
      "7232\n",
      "7296\n",
      "7360\n",
      "7424\n",
      "7488\n",
      "7552\n",
      "7616\n",
      "7680\n",
      "7744\n",
      "7808\n",
      "7872\n",
      "7936\n",
      "8000\n",
      "8064\n",
      "8128\n",
      "8192\n",
      "8256\n",
      "8320\n",
      "8384\n",
      "8448\n",
      "8512\n",
      "8576\n",
      "8640\n",
      "8704\n",
      "8768\n",
      "8832\n",
      "8896\n",
      "8960\n",
      "9024\n",
      "9088\n",
      "9152\n",
      "9216\n",
      "9280\n",
      "9344\n",
      "9408\n",
      "9472\n",
      "9536\n",
      "9600\n",
      "9664\n",
      "9728\n",
      "9792\n",
      "9856\n",
      "9920\n",
      "9984\n",
      "10048\n",
      "10112\n",
      "10176\n",
      "10240\n",
      "10304\n",
      "10368\n",
      "10432\n",
      "10496\n",
      "10560\n",
      "10624\n",
      "10688\n",
      "10752\n",
      "10816\n",
      "10880\n",
      "10944\n",
      "11008\n",
      "11072\n",
      "11136\n",
      "11200\n",
      "11264\n",
      "11328\n",
      "11392\n",
      "11456\n",
      "11520\n",
      "11584\n",
      "11648\n",
      "11712\n",
      "11776\n",
      "11840\n",
      "11904\n",
      "11968\n",
      "12032\n",
      "12096\n",
      "12160\n",
      "12224\n",
      "12288\n",
      "12352\n",
      "12416\n",
      "12480\n",
      "12544\n",
      "12608\n",
      "12672\n",
      "12736\n",
      "12800\n",
      "12864\n",
      "12928\n",
      "12992\n",
      "13056\n",
      "13120\n",
      "13184\n",
      "13248\n",
      "13312\n",
      "13376\n",
      "13440\n",
      "13504\n",
      "13568\n",
      "13632\n",
      "13696\n",
      "13760\n",
      "13824\n",
      "13888\n",
      "13952\n",
      "14016\n",
      "14080\n",
      "14144\n",
      "14208\n",
      "14272\n",
      "14336\n",
      "14400\n",
      "14464\n",
      "14528\n",
      "14592\n",
      "14656\n",
      "14720\n",
      "14784\n",
      "14848\n",
      "14912\n",
      "14976\n",
      "15040\n",
      "15104\n",
      "15168\n",
      "15232\n",
      "15296\n",
      "15360\n",
      "15424\n",
      "15488\n",
      "15552\n",
      "15616\n",
      "15680\n",
      "15744\n",
      "15808\n",
      "15872\n",
      "15936\n",
      "16000\n",
      "16064\n",
      "16128\n",
      "16192\n",
      "16256\n",
      "16320\n",
      "16384\n",
      "16448\n",
      "16512\n",
      "16576\n",
      "16640\n",
      "16704\n",
      "16768\n",
      "16832\n",
      "16896\n",
      "16960\n",
      "17024\n",
      "17088\n",
      "17152\n",
      "17216\n",
      "17280\n",
      "17344\n",
      "17408\n",
      "17472\n",
      "17536\n",
      "17600\n",
      "17664\n",
      "17728\n",
      "17792\n",
      "17856\n",
      "17920\n",
      "17984\n",
      "18048\n",
      "18112\n",
      "18176\n",
      "18240\n",
      "18304\n",
      "18368\n",
      "18432\n",
      "18496\n",
      "18560\n",
      "18624\n",
      "18688\n",
      "18752\n",
      "18816\n",
      "18880\n",
      "18944\n",
      "19008\n",
      "19072\n",
      "19136\n",
      "19200\n",
      "19264\n",
      "19328\n",
      "19392\n",
      "19456\n",
      "19520\n",
      "19584\n",
      "19648\n",
      "19712\n",
      "19776\n",
      "19840\n",
      "19904\n",
      "19968\n",
      "20032\n",
      "20096\n",
      "20160\n",
      "20224\n",
      "20288\n",
      "20352\n",
      "20416\n",
      "20480\n",
      "20544\n",
      "20608\n",
      "20672\n",
      "20736\n",
      "20800\n",
      "20864\n",
      "20928\n",
      "20992\n",
      "21056\n",
      "21120\n",
      "21184\n",
      "21248\n",
      "21312\n",
      "21376\n",
      "21440\n",
      "21504\n",
      "21568\n",
      "21632\n",
      "21696\n",
      "21760\n",
      "21824\n",
      "21888\n",
      "21952\n",
      "22016\n",
      "22080\n",
      "22144\n",
      "22208\n",
      "22272\n",
      "22336\n",
      "22400\n",
      "22464\n",
      "22528\n",
      "22592\n",
      "22656\n",
      "22720\n",
      "22784\n",
      "22848\n",
      "22912\n",
      "22976\n",
      "23040\n",
      "23104\n",
      "23168\n",
      "23232\n",
      "23296\n",
      "23360\n",
      "23424\n",
      "23488\n",
      "23552\n",
      "23616\n",
      "23680\n",
      "23744\n",
      "23808\n",
      "23872\n",
      "23936\n",
      "24000\n",
      "24064\n",
      "24128\n",
      "24192\n",
      "24256\n",
      "24320\n",
      "24384\n",
      "24448\n",
      "24512\n",
      "24576\n",
      "24640\n",
      "24704\n",
      "24768\n",
      "24832\n",
      "24896\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for (i, batch) in enumerate(train_loader):\n",
    "    counter += 64\n",
    "    print(counter)\n",
    "    # import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, fashion.n_classes)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.4583 Acc: 0.8455\n",
      "val Loss: 0.3618 Acc: 0.8743\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2945 Acc: 0.8990\n",
      "val Loss: 0.3084 Acc: 0.8938\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2383 Acc: 0.9148\n",
      "val Loss: 0.2789 Acc: 0.9048\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2158 Acc: 0.9236\n",
      "val Loss: 0.2584 Acc: 0.9139\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1872 Acc: 0.9337\n",
      "val Loss: 0.2625 Acc: 0.9118\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1727 Acc: 0.9376\n",
      "val Loss: 0.2493 Acc: 0.9216\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1543 Acc: 0.9456\n",
      "val Loss: 0.2745 Acc: 0.9111\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.0944 Acc: 0.9653\n",
      "val Loss: 0.1934 Acc: 0.9382\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0764 Acc: 0.9736\n",
      "val Loss: 0.1966 Acc: 0.9414\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0645 Acc: 0.9779\n",
      "val Loss: 0.2009 Acc: 0.9368\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0519 Acc: 0.9817\n",
      "val Loss: 0.2134 Acc: 0.9382\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0453 Acc: 0.9854\n",
      "val Loss: 0.2272 Acc: 0.9365\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0342 Acc: 0.9893\n",
      "val Loss: 0.2524 Acc: 0.9365\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0285 Acc: 0.9908\n",
      "val Loss: 0.2805 Acc: 0.9344\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0216 Acc: 0.9934\n",
      "val Loss: 0.2607 Acc: 0.9373\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0177 Acc: 0.9950\n",
      "val Loss: 0.2760 Acc: 0.9374\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0181 Acc: 0.9946\n",
      "val Loss: 0.2731 Acc: 0.9388\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0154 Acc: 0.9956\n",
      "val Loss: 0.2826 Acc: 0.9391\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0142 Acc: 0.9959\n",
      "val Loss: 0.2754 Acc: 0.9389\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0144 Acc: 0.9959\n",
      "val Loss: 0.2893 Acc: 0.9385\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0133 Acc: 0.9963\n",
      "val Loss: 0.2995 Acc: 0.9377\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0118 Acc: 0.9972\n",
      "val Loss: 0.2929 Acc: 0.9376\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0123 Acc: 0.9966\n",
      "val Loss: 0.2870 Acc: 0.9383\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0117 Acc: 0.9965\n",
      "val Loss: 0.2910 Acc: 0.9392\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0107 Acc: 0.9973\n",
      "val Loss: 0.2956 Acc: 0.9364\n",
      "\n",
      "Training complete in 92m 23s\n",
      "Best val Acc: 0.941361\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
